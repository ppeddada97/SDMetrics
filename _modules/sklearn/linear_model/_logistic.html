

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>sklearn.linear_model._logistic &mdash; SDMetrics 0.3.0 documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> SDMetrics
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../install.html#stable-release">Stable Release</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../install.html#from-source">From Source</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../quickstart.html">Quickstart</a></li>
</ul>
<p class="caption"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/sdmetrics.html">API Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/sdmetrics.html#subpackages">Subpackages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/sdmetrics.column_pairs.html">sdmetrics.column_pairs package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api/sdmetrics.column_pairs.html#subpackages">Subpackages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api/sdmetrics.column_pairs.html#submodules">Submodules</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/sdmetrics.multi_table.html">sdmetrics.multi_table package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api/sdmetrics.multi_table.html#subpackages">Subpackages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api/sdmetrics.multi_table.html#submodules">Submodules</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/sdmetrics.single_column.html">sdmetrics.single_column package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api/sdmetrics.single_column.html#subpackages">Subpackages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api/sdmetrics.single_column.html#submodules">Submodules</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/sdmetrics.single_table.html">sdmetrics.single_table package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api/sdmetrics.single_table.html#subpackages">Subpackages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api/sdmetrics.single_table.html#submodules">Submodules</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/sdmetrics.timeseries.html">sdmetrics.timeseries package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api/sdmetrics.timeseries.html#subpackages">Subpackages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api/sdmetrics.timeseries.html#submodules">Submodules</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/sdmetrics.html#submodules">Submodules</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/sdmetrics.base.html">sdmetrics.base module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/sdmetrics.demos.html">sdmetrics.demos module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/sdmetrics.goal.html">sdmetrics.goal module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/sdmetrics.utils.html">sdmetrics.utils module</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../contributing.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#types-of-contributions">Types of Contributions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#report-bugs">Report Bugs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#fix-bugs">Fix Bugs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#implement-features">Implement Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#write-documentation">Write Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#submit-feedback">Submit Feedback</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#get-started">Get Started!</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#pull-request-guidelines">Pull Request Guidelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#unit-testing-guidelines">Unit Testing Guidelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#tips">Tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#release-workflow">Release Workflow</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#release-candidates">Release Candidates</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../authors.html">Credits</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../history.html">History</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#v0-3-0-2021-03-30">v0.3.0 - 2021-03-30</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#issues-closed">Issues closed</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#v0-2-0-2021-02-24">v0.2.0 - 2021-02-24</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#v0-1-3-2021-02-13">v0.1.3 - 2021-02-13</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id3">Issues closed</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#v0-1-2-2021-01-27">v0.1.2 - 2021-01-27</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id5">Issues closed</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#v0-1-1-2020-12-30">v0.1.1 - 2020-12-30</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id10">Issues closed</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#v0-1-0-2020-12-18">v0.1.0 - 2020-12-18</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#v0-0-4-2020-11-27">v0.0.4 - 2020-11-27</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#v0-0-3-2020-11-20">v0.0.3 - 2020-11-20</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id14">Issues closed</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#v0-0-2-2020-08-08">v0.0.2 - 2020-08-08</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#v0-0-1-2020-06-26">v0.0.1 - 2020-06-26</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">SDMetrics</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>sklearn.linear_model._logistic</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for sklearn.linear_model._logistic</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Logistic Regression</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># Author: Gael Varoquaux &lt;gael.varoquaux@normalesup.org&gt;</span>
<span class="c1">#         Fabian Pedregosa &lt;f@bianp.net&gt;</span>
<span class="c1">#         Alexandre Gramfort &lt;alexandre.gramfort@telecom-paristech.fr&gt;</span>
<span class="c1">#         Manoj Kumar &lt;manojkumarsivaraj334@gmail.com&gt;</span>
<span class="c1">#         Lars Buitinck</span>
<span class="c1">#         Simon Wu &lt;s8wu@uwaterloo.ca&gt;</span>
<span class="c1">#         Arthur Mensch &lt;arthur.mensch@m4x.org</span>

<span class="kn">import</span> <span class="nn">numbers</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">optimize</span><span class="p">,</span> <span class="n">sparse</span>
<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">expit</span><span class="p">,</span> <span class="n">logsumexp</span>
<span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">effective_n_jobs</span>

<span class="kn">from</span> <span class="nn">._base</span> <span class="kn">import</span> <span class="n">LinearClassifierMixin</span><span class="p">,</span> <span class="n">SparseCoefMixin</span><span class="p">,</span> <span class="n">BaseEstimator</span>
<span class="kn">from</span> <span class="nn">._sag</span> <span class="kn">import</span> <span class="n">sag_solver</span>
<span class="kn">from</span> <span class="nn">..preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span><span class="p">,</span> <span class="n">LabelBinarizer</span>
<span class="kn">from</span> <span class="nn">..svm._base</span> <span class="kn">import</span> <span class="n">_fit_liblinear</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="kn">import</span> <span class="n">check_array</span><span class="p">,</span> <span class="n">check_consistent_length</span><span class="p">,</span> <span class="n">compute_class_weight</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="kn">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">..utils.extmath</span> <span class="kn">import</span> <span class="p">(</span><span class="n">log_logistic</span><span class="p">,</span> <span class="n">safe_sparse_dot</span><span class="p">,</span> <span class="n">softmax</span><span class="p">,</span>
                             <span class="n">squared_norm</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">..utils.extmath</span> <span class="kn">import</span> <span class="n">row_norms</span>
<span class="kn">from</span> <span class="nn">..utils.optimize</span> <span class="kn">import</span> <span class="n">_newton_cg</span><span class="p">,</span> <span class="n">_check_optimize_result</span>
<span class="kn">from</span> <span class="nn">..utils.validation</span> <span class="kn">import</span> <span class="n">check_is_fitted</span><span class="p">,</span> <span class="n">_check_sample_weight</span>
<span class="kn">from</span> <span class="nn">..utils.validation</span> <span class="kn">import</span> <span class="n">_deprecate_positional_args</span>
<span class="kn">from</span> <span class="nn">..utils.multiclass</span> <span class="kn">import</span> <span class="n">check_classification_targets</span>
<span class="kn">from</span> <span class="nn">..utils.fixes</span> <span class="kn">import</span> <span class="n">_joblib_parallel_args</span>
<span class="kn">from</span> <span class="nn">..utils.fixes</span> <span class="kn">import</span> <span class="n">delayed</span>
<span class="kn">from</span> <span class="nn">..model_selection</span> <span class="kn">import</span> <span class="n">check_cv</span>
<span class="kn">from</span> <span class="nn">..metrics</span> <span class="kn">import</span> <span class="n">get_scorer</span>


<span class="n">_LOGISTIC_SOLVER_CONVERGENCE_MSG</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;Please also refer to the documentation for alternative solver options:</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="s2">&quot;    https://scikit-learn.org/stable/modules/linear_model.html&quot;</span>
    <span class="s2">&quot;#logistic-regression&quot;</span><span class="p">)</span>


<span class="c1"># .. some helper functions for logistic_regression_path ..</span>
<span class="k">def</span> <span class="nf">_intercept_dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes y * np.dot(X, w).</span>

<span class="sd">    It takes into consideration if the intercept should be fit or not.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    w : ndarray of shape (n_features,) or (n_features + 1,)</span>
<span class="sd">        Coefficient vector.</span>

<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        Training data.</span>

<span class="sd">    y : ndarray of shape (n_samples,)</span>
<span class="sd">        Array of labels.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    w : ndarray of shape (n_features,)</span>
<span class="sd">        Coefficient vector without the intercept weight (w[-1]) if the</span>
<span class="sd">        intercept should be fit. Unchanged otherwise.</span>

<span class="sd">    c : float</span>
<span class="sd">        The intercept.</span>

<span class="sd">    yz : float</span>
<span class="sd">        y * np.dot(X, w).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">c</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="k">if</span> <span class="n">w</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">z</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">c</span>
    <span class="n">yz</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span>
    <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">yz</span>


<span class="k">def</span> <span class="nf">_logistic_loss_and_grad</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the logistic loss and gradient.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    w : ndarray of shape (n_features,) or (n_features + 1,)</span>
<span class="sd">        Coefficient vector.</span>

<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        Training data.</span>

<span class="sd">    y : ndarray of shape (n_samples,)</span>
<span class="sd">        Array of labels.</span>

<span class="sd">    alpha : float</span>
<span class="sd">        Regularization parameter. alpha is equal to 1 / C.</span>

<span class="sd">    sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">        Array of weights that are assigned to individual samples.</span>
<span class="sd">        If not provided, then each sample is given unit weight.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    out : float</span>
<span class="sd">        Logistic loss.</span>

<span class="sd">    grad : ndarray of shape (n_features,) or (n_features + 1,)</span>
<span class="sd">        Logistic gradient.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

    <span class="n">w</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">yz</span> <span class="o">=</span> <span class="n">_intercept_dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Logistic loss is the negative of the log of the logistic function.</span>
    <span class="n">out</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span> <span class="o">*</span> <span class="n">log_logistic</span><span class="p">(</span><span class="n">yz</span><span class="p">))</span> <span class="o">+</span> <span class="o">.</span><span class="mi">5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

    <span class="n">z</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span><span class="n">yz</span><span class="p">)</span>
    <span class="n">z0</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">y</span>

    <span class="n">grad</span><span class="p">[:</span><span class="n">n_features</span><span class="p">]</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">z0</span><span class="p">)</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">w</span>

    <span class="c1"># Case where we fit the intercept.</span>
    <span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">n_features</span><span class="p">:</span>
        <span class="n">grad</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">z0</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">grad</span>


<span class="k">def</span> <span class="nf">_logistic_loss</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the logistic loss.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    w : ndarray of shape (n_features,) or (n_features + 1,)</span>
<span class="sd">        Coefficient vector.</span>

<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        Training data.</span>

<span class="sd">    y : ndarray of shape (n_samples,)</span>
<span class="sd">        Array of labels.</span>

<span class="sd">    alpha : float</span>
<span class="sd">        Regularization parameter. alpha is equal to 1 / C.</span>

<span class="sd">    sample_weight : array-like of shape (n_samples,) default=None</span>
<span class="sd">        Array of weights that are assigned to individual samples.</span>
<span class="sd">        If not provided, then each sample is given unit weight.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    out : float</span>
<span class="sd">        Logistic loss.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">w</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">yz</span> <span class="o">=</span> <span class="n">_intercept_dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># Logistic loss is the negative of the log of the logistic function.</span>
    <span class="n">out</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span> <span class="o">*</span> <span class="n">log_logistic</span><span class="p">(</span><span class="n">yz</span><span class="p">))</span> <span class="o">+</span> <span class="o">.</span><span class="mi">5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">_logistic_grad_hess</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the gradient and the Hessian, in the case of a logistic loss.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    w : ndarray of shape (n_features,) or (n_features + 1,)</span>
<span class="sd">        Coefficient vector.</span>

<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        Training data.</span>

<span class="sd">    y : ndarray of shape (n_samples,)</span>
<span class="sd">        Array of labels.</span>

<span class="sd">    alpha : float</span>
<span class="sd">        Regularization parameter. alpha is equal to 1 / C.</span>

<span class="sd">    sample_weight : array-like of shape (n_samples,) default=None</span>
<span class="sd">        Array of weights that are assigned to individual samples.</span>
<span class="sd">        If not provided, then each sample is given unit weight.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    grad : ndarray of shape (n_features,) or (n_features + 1,)</span>
<span class="sd">        Logistic gradient.</span>

<span class="sd">    Hs : callable</span>
<span class="sd">        Function that takes the gradient as a parameter and returns the</span>
<span class="sd">        matrix product of the Hessian and gradient.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">n_features</span>

    <span class="n">w</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">yz</span> <span class="o">=</span> <span class="n">_intercept_dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">z</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span><span class="n">yz</span><span class="p">)</span>
    <span class="n">z0</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">y</span>

    <span class="n">grad</span><span class="p">[:</span><span class="n">n_features</span><span class="p">]</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">z0</span><span class="p">)</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">w</span>

    <span class="c1"># Case where we fit the intercept.</span>
    <span class="k">if</span> <span class="n">fit_intercept</span><span class="p">:</span>
        <span class="n">grad</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">z0</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="c1"># The mat-vec product of the Hessian</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="n">z</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">z</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="n">dX</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">sparse</span><span class="o">.</span><span class="n">dia_matrix</span><span class="p">((</span><span class="n">d</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                             <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)),</span> <span class="n">X</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Precompute as much as possible</span>
        <span class="n">dX</span> <span class="o">=</span> <span class="n">d</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span>

    <span class="k">if</span> <span class="n">fit_intercept</span><span class="p">:</span>
        <span class="c1"># Calculate the double derivative with respect to intercept</span>
        <span class="c1"># In the case of sparse matrices this returns a matrix object.</span>
        <span class="n">dd_intercept</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dX</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">Hs</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">ret</span><span class="p">[:</span><span class="n">n_features</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dX</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">s</span><span class="p">[:</span><span class="n">n_features</span><span class="p">]))</span>
        <span class="n">ret</span><span class="p">[:</span><span class="n">n_features</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">s</span><span class="p">[:</span><span class="n">n_features</span><span class="p">]</span>

        <span class="c1"># For the fit intercept case.</span>
        <span class="k">if</span> <span class="n">fit_intercept</span><span class="p">:</span>
            <span class="n">ret</span><span class="p">[:</span><span class="n">n_features</span><span class="p">]</span> <span class="o">+=</span> <span class="n">s</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">dd_intercept</span>
            <span class="n">ret</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">dd_intercept</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">s</span><span class="p">[:</span><span class="n">n_features</span><span class="p">])</span>
            <span class="n">ret</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">d</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">*</span> <span class="n">s</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">ret</span>

    <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">Hs</span>


<span class="k">def</span> <span class="nf">_multinomial_loss</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes multinomial loss and class probabilities.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    w : ndarray of shape (n_classes * n_features,) or</span>
<span class="sd">        (n_classes * (n_features + 1),)</span>
<span class="sd">        Coefficient vector.</span>

<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        Training data.</span>

<span class="sd">    Y : ndarray of shape (n_samples, n_classes)</span>
<span class="sd">        Transformed labels according to the output of LabelBinarizer.</span>

<span class="sd">    alpha : float</span>
<span class="sd">        Regularization parameter. alpha is equal to 1 / C.</span>

<span class="sd">    sample_weight : array-like of shape (n_samples,)</span>
<span class="sd">        Array of weights that are assigned to individual samples.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss : float</span>
<span class="sd">        Multinomial loss.</span>

<span class="sd">    p : ndarray of shape (n_samples, n_classes)</span>
<span class="sd">        Estimated class probabilities.</span>

<span class="sd">    w : ndarray of shape (n_classes, n_features)</span>
<span class="sd">        Reshaped param vector excluding intercept terms.</span>

<span class="sd">    Reference</span>
<span class="sd">    ---------</span>
<span class="sd">    Bishop, C. M. (2006). Pattern recognition and machine learning.</span>
<span class="sd">    Springer. (Chapter 4.3.4)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_classes</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="p">(</span><span class="n">n_classes</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_features</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">fit_intercept</span><span class="p">:</span>
        <span class="n">intercept</span> <span class="o">=</span> <span class="n">w</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">intercept</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">+=</span> <span class="n">intercept</span>
    <span class="n">p</span> <span class="o">-=</span> <span class="n">logsumexp</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">sample_weight</span> <span class="o">*</span> <span class="n">Y</span> <span class="o">*</span> <span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">squared_norm</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">w</span>


<span class="k">def</span> <span class="nf">_multinomial_loss_grad</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the multinomial loss, gradient and class probabilities.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    w : ndarray of shape (n_classes * n_features,) or</span>
<span class="sd">        (n_classes * (n_features + 1),)</span>
<span class="sd">        Coefficient vector.</span>

<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        Training data.</span>

<span class="sd">    Y : ndarray of shape (n_samples, n_classes)</span>
<span class="sd">        Transformed labels according to the output of LabelBinarizer.</span>

<span class="sd">    alpha : float</span>
<span class="sd">        Regularization parameter. alpha is equal to 1 / C.</span>

<span class="sd">    sample_weight : array-like of shape (n_samples,)</span>
<span class="sd">        Array of weights that are assigned to individual samples.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss : float</span>
<span class="sd">        Multinomial loss.</span>

<span class="sd">    grad : ndarray of shape (n_classes * n_features,) or \</span>
<span class="sd">            (n_classes * (n_features + 1),)</span>
<span class="sd">        Ravelled gradient of the multinomial loss.</span>

<span class="sd">    p : ndarray of shape (n_samples, n_classes)</span>
<span class="sd">        Estimated class probabilities</span>

<span class="sd">    Reference</span>
<span class="sd">    ---------</span>
<span class="sd">    Bishop, C. M. (2006). Pattern recognition and machine learning.</span>
<span class="sd">    Springer. (Chapter 4.3.4)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_classes</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">fit_intercept</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="n">n_classes</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_features</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">+</span> <span class="nb">bool</span><span class="p">(</span><span class="n">fit_intercept</span><span class="p">)),</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">_multinomial_loss</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
    <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="p">(</span><span class="n">p</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span>
    <span class="n">grad</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_features</span><span class="p">]</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">diff</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="n">grad</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_features</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">w</span>
    <span class="k">if</span> <span class="n">fit_intercept</span><span class="p">:</span>
        <span class="n">grad</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">diff</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">p</span>


<span class="k">def</span> <span class="nf">_multinomial_grad_hess</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradient and the Hessian, in the case of a multinomial loss.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    w : ndarray of shape (n_classes * n_features,) or</span>
<span class="sd">        (n_classes * (n_features + 1),)</span>
<span class="sd">        Coefficient vector.</span>

<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        Training data.</span>

<span class="sd">    Y : ndarray of shape (n_samples, n_classes)</span>
<span class="sd">        Transformed labels according to the output of LabelBinarizer.</span>

<span class="sd">    alpha : float</span>
<span class="sd">        Regularization parameter. alpha is equal to 1 / C.</span>

<span class="sd">    sample_weight : array-like of shape (n_samples,)</span>
<span class="sd">        Array of weights that are assigned to individual samples.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    grad : ndarray of shape (n_classes * n_features,) or \</span>
<span class="sd">            (n_classes * (n_features + 1),)</span>
<span class="sd">        Ravelled gradient of the multinomial loss.</span>

<span class="sd">    hessp : callable</span>
<span class="sd">        Function that takes in a vector input of shape (n_classes * n_features)</span>
<span class="sd">        or (n_classes * (n_features + 1)) and returns matrix-vector product</span>
<span class="sd">        with hessian.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Barak A. Pearlmutter (1993). Fast Exact Multiplication by the Hessian.</span>
<span class="sd">        http://www.bcl.hamilton.ie/~barak/papers/nc-hessian.pdf</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">n_classes</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="p">(</span><span class="n">n_classes</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_features</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># `loss` is unused. Refactoring to avoid computing it does not</span>
    <span class="c1"># significantly speed up the computation and decreases readability</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">_multinomial_loss_grad</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
    <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

    <span class="c1"># Hessian-vector product derived by applying the R-operator on the gradient</span>
    <span class="c1"># of the multinomial loss function.</span>
    <span class="k">def</span> <span class="nf">hessp</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">fit_intercept</span><span class="p">:</span>
            <span class="n">inter_terms</span> <span class="o">=</span> <span class="n">v</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">inter_terms</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># r_yhat holds the result of applying the R-operator on the multinomial</span>
        <span class="c1"># estimator.</span>
        <span class="n">r_yhat</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">r_yhat</span> <span class="o">+=</span> <span class="n">inter_terms</span>
        <span class="n">r_yhat</span> <span class="o">+=</span> <span class="p">(</span><span class="o">-</span><span class="n">p</span> <span class="o">*</span> <span class="n">r_yhat</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
        <span class="n">r_yhat</span> <span class="o">*=</span> <span class="n">p</span>
        <span class="n">r_yhat</span> <span class="o">*=</span> <span class="n">sample_weight</span>
        <span class="n">hessProd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">+</span> <span class="nb">bool</span><span class="p">(</span><span class="n">fit_intercept</span><span class="p">)))</span>
        <span class="n">hessProd</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_features</span><span class="p">]</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">r_yhat</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
        <span class="n">hessProd</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_features</span><span class="p">]</span> <span class="o">+=</span> <span class="n">v</span> <span class="o">*</span> <span class="n">alpha</span>
        <span class="k">if</span> <span class="n">fit_intercept</span><span class="p">:</span>
            <span class="n">hessProd</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">r_yhat</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hessProd</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hessp</span>


<span class="k">def</span> <span class="nf">_check_solver</span><span class="p">(</span><span class="n">solver</span><span class="p">,</span> <span class="n">penalty</span><span class="p">,</span> <span class="n">dual</span><span class="p">):</span>
    <span class="n">all_solvers</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;liblinear&#39;</span><span class="p">,</span> <span class="s1">&#39;newton-cg&#39;</span><span class="p">,</span> <span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="s1">&#39;sag&#39;</span><span class="p">,</span> <span class="s1">&#39;saga&#39;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">solver</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">all_solvers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Logistic Regression supports only solvers in </span><span class="si">%s</span><span class="s2">, got&quot;</span>
                         <span class="s2">&quot; </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">all_solvers</span><span class="p">,</span> <span class="n">solver</span><span class="p">))</span>

    <span class="n">all_penalties</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;elasticnet&#39;</span><span class="p">,</span> <span class="s1">&#39;none&#39;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">penalty</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">all_penalties</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Logistic Regression supports only penalties in </span><span class="si">%s</span><span class="s2">,&quot;</span>
                         <span class="s2">&quot; got </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">all_penalties</span><span class="p">,</span> <span class="n">penalty</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">solver</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;liblinear&#39;</span><span class="p">,</span> <span class="s1">&#39;saga&#39;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">penalty</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;none&#39;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Solver </span><span class="si">%s</span><span class="s2"> supports only &#39;l2&#39; or &#39;none&#39; penalties, &quot;</span>
                         <span class="s2">&quot;got </span><span class="si">%s</span><span class="s2"> penalty.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">solver</span><span class="p">,</span> <span class="n">penalty</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">solver</span> <span class="o">!=</span> <span class="s1">&#39;liblinear&#39;</span> <span class="ow">and</span> <span class="n">dual</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Solver </span><span class="si">%s</span><span class="s2"> supports only &quot;</span>
                         <span class="s2">&quot;dual=False, got dual=</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">solver</span><span class="p">,</span> <span class="n">dual</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">penalty</span> <span class="o">==</span> <span class="s1">&#39;elasticnet&#39;</span> <span class="ow">and</span> <span class="n">solver</span> <span class="o">!=</span> <span class="s1">&#39;saga&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Only &#39;saga&#39; solver supports elasticnet penalty,&quot;</span>
                         <span class="s2">&quot; got solver=</span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">solver</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;liblinear&#39;</span> <span class="ow">and</span> <span class="n">penalty</span> <span class="o">==</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;penalty=&#39;none&#39; is not supported for the liblinear solver&quot;</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">solver</span>


<span class="k">def</span> <span class="nf">_check_multi_class</span><span class="p">(</span><span class="n">multi_class</span><span class="p">,</span> <span class="n">solver</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;auto&#39;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;liblinear&#39;</span><span class="p">:</span>
            <span class="n">multi_class</span> <span class="o">=</span> <span class="s1">&#39;ovr&#39;</span>
        <span class="k">elif</span> <span class="n">n_classes</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">multi_class</span> <span class="o">=</span> <span class="s1">&#39;multinomial&#39;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">multi_class</span> <span class="o">=</span> <span class="s1">&#39;ovr&#39;</span>
    <span class="k">if</span> <span class="n">multi_class</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;multinomial&#39;</span><span class="p">,</span> <span class="s1">&#39;ovr&#39;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;multi_class should be &#39;multinomial&#39;, &#39;ovr&#39; or &quot;</span>
                         <span class="s2">&quot;&#39;auto&#39;. Got </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="n">multi_class</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span> <span class="ow">and</span> <span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;liblinear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Solver </span><span class="si">%s</span><span class="s2"> does not support &quot;</span>
                         <span class="s2">&quot;a multinomial backend.&quot;</span> <span class="o">%</span> <span class="n">solver</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">multi_class</span>


<span class="k">def</span> <span class="nf">_logistic_regression_path</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pos_class</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">Cs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                              <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                              <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">coef</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span>
                              <span class="n">intercept_scaling</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span>
                              <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                              <span class="n">max_squared_sum</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">l1_ratio</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute a Logistic Regression model for a list of regularization</span>
<span class="sd">    parameters.</span>

<span class="sd">    This is an implementation that uses the result of the previous model</span>
<span class="sd">    to speed up computations along the set of solutions, making it faster</span>
<span class="sd">    than sequentially calling LogisticRegression for the different parameters.</span>
<span class="sd">    Note that there will be no speedup with liblinear solver, since it does</span>
<span class="sd">    not handle warm-starting.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;logistic_regression&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        Input data.</span>

<span class="sd">    y : array-like of shape (n_samples,) or (n_samples, n_targets)</span>
<span class="sd">        Input data, target values.</span>

<span class="sd">    pos_class : int, default=None</span>
<span class="sd">        The class with respect to which we perform a one-vs-all fit.</span>
<span class="sd">        If None, then it is assumed that the given problem is binary.</span>

<span class="sd">    Cs : int or array-like of shape (n_cs,), default=10</span>
<span class="sd">        List of values for the regularization parameter or integer specifying</span>
<span class="sd">        the number of regularization parameters that should be used. In this</span>
<span class="sd">        case, the parameters will be chosen in a logarithmic scale between</span>
<span class="sd">        1e-4 and 1e4.</span>

<span class="sd">    fit_intercept : bool, default=True</span>
<span class="sd">        Whether to fit an intercept for the model. In this case the shape of</span>
<span class="sd">        the returned array is (n_cs, n_features + 1).</span>

<span class="sd">    max_iter : int, default=100</span>
<span class="sd">        Maximum number of iterations for the solver.</span>

<span class="sd">    tol : float, default=1e-4</span>
<span class="sd">        Stopping criterion. For the newton-cg and lbfgs solvers, the iteration</span>
<span class="sd">        will stop when ``max{|g_i | i = 1, ..., n} &lt;= tol``</span>
<span class="sd">        where ``g_i`` is the i-th component of the gradient.</span>

<span class="sd">    verbose : int, default=0</span>
<span class="sd">        For the liblinear and lbfgs solvers set verbose to any positive</span>
<span class="sd">        number for verbosity.</span>

<span class="sd">    solver : {&#39;lbfgs&#39;, &#39;newton-cg&#39;, &#39;liblinear&#39;, &#39;sag&#39;, &#39;saga&#39;}, \</span>
<span class="sd">            default=&#39;lbfgs&#39;</span>
<span class="sd">        Numerical solver to use.</span>

<span class="sd">    coef : array-like of shape (n_features,), default=None</span>
<span class="sd">        Initialization value for coefficients of logistic regression.</span>
<span class="sd">        Useless for liblinear solver.</span>

<span class="sd">    class_weight : dict or &#39;balanced&#39;, default=None</span>
<span class="sd">        Weights associated with classes in the form ``{class_label: weight}``.</span>
<span class="sd">        If not given, all classes are supposed to have weight one.</span>

<span class="sd">        The &quot;balanced&quot; mode uses the values of y to automatically adjust</span>
<span class="sd">        weights inversely proportional to class frequencies in the input data</span>
<span class="sd">        as ``n_samples / (n_classes * np.bincount(y))``.</span>

<span class="sd">        Note that these weights will be multiplied with sample_weight (passed</span>
<span class="sd">        through the fit method) if sample_weight is specified.</span>

<span class="sd">    dual : bool, default=False</span>
<span class="sd">        Dual or primal formulation. Dual formulation is only implemented for</span>
<span class="sd">        l2 penalty with liblinear solver. Prefer dual=False when</span>
<span class="sd">        n_samples &gt; n_features.</span>

<span class="sd">    penalty : {&#39;l1&#39;, &#39;l2&#39;, &#39;elasticnet&#39;}, default=&#39;l2&#39;</span>
<span class="sd">        Used to specify the norm used in the penalization. The &#39;newton-cg&#39;,</span>
<span class="sd">        &#39;sag&#39; and &#39;lbfgs&#39; solvers support only l2 penalties. &#39;elasticnet&#39; is</span>
<span class="sd">        only supported by the &#39;saga&#39; solver.</span>

<span class="sd">    intercept_scaling : float, default=1.</span>
<span class="sd">        Useful only when the solver &#39;liblinear&#39; is used</span>
<span class="sd">        and self.fit_intercept is set to True. In this case, x becomes</span>
<span class="sd">        [x, self.intercept_scaling],</span>
<span class="sd">        i.e. a &quot;synthetic&quot; feature with constant value equal to</span>
<span class="sd">        intercept_scaling is appended to the instance vector.</span>
<span class="sd">        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.</span>

<span class="sd">        Note! the synthetic feature weight is subject to l1/l2 regularization</span>
<span class="sd">        as all other features.</span>
<span class="sd">        To lessen the effect of regularization on synthetic feature weight</span>
<span class="sd">        (and therefore on the intercept) intercept_scaling has to be increased.</span>

<span class="sd">    multi_class : {&#39;ovr&#39;, &#39;multinomial&#39;, &#39;auto&#39;}, default=&#39;auto&#39;</span>
<span class="sd">        If the option chosen is &#39;ovr&#39;, then a binary problem is fit for each</span>
<span class="sd">        label. For &#39;multinomial&#39; the loss minimised is the multinomial loss fit</span>
<span class="sd">        across the entire probability distribution, *even when the data is</span>
<span class="sd">        binary*. &#39;multinomial&#39; is unavailable when solver=&#39;liblinear&#39;.</span>
<span class="sd">        &#39;auto&#39; selects &#39;ovr&#39; if the data is binary, or if solver=&#39;liblinear&#39;,</span>
<span class="sd">        and otherwise selects &#39;multinomial&#39;.</span>

<span class="sd">        .. versionadded:: 0.18</span>
<span class="sd">           Stochastic Average Gradient descent solver for &#39;multinomial&#39; case.</span>
<span class="sd">        .. versionchanged:: 0.22</span>
<span class="sd">            Default changed from &#39;ovr&#39; to &#39;auto&#39; in 0.22.</span>

<span class="sd">    random_state : int, RandomState instance, default=None</span>
<span class="sd">        Used when ``solver`` == &#39;sag&#39;, &#39;saga&#39; or &#39;liblinear&#39; to shuffle the</span>
<span class="sd">        data. See :term:`Glossary &lt;random_state&gt;` for details.</span>

<span class="sd">    check_input : bool, default=True</span>
<span class="sd">        If False, the input arrays X and y will not be checked.</span>

<span class="sd">    max_squared_sum : float, default=None</span>
<span class="sd">        Maximum squared sum of X over samples. Used only in SAG solver.</span>
<span class="sd">        If None, it will be computed, going through all the samples.</span>
<span class="sd">        The value should be precomputed to speed up cross validation.</span>

<span class="sd">    sample_weight : array-like of shape(n_samples,), default=None</span>
<span class="sd">        Array of weights that are assigned to individual samples.</span>
<span class="sd">        If not provided, then each sample is given unit weight.</span>

<span class="sd">    l1_ratio : float, default=None</span>
<span class="sd">        The Elastic-Net mixing parameter, with ``0 &lt;= l1_ratio &lt;= 1``. Only</span>
<span class="sd">        used if ``penalty=&#39;elasticnet&#39;``. Setting ``l1_ratio=0`` is equivalent</span>
<span class="sd">        to using ``penalty=&#39;l2&#39;``, while setting ``l1_ratio=1`` is equivalent</span>
<span class="sd">        to using ``penalty=&#39;l1&#39;``. For ``0 &lt; l1_ratio &lt;1``, the penalty is a</span>
<span class="sd">        combination of L1 and L2.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    coefs : ndarray of shape (n_cs, n_features) or (n_cs, n_features + 1)</span>
<span class="sd">        List of coefficients for the Logistic Regression model. If</span>
<span class="sd">        fit_intercept is set to True then the second dimension will be</span>
<span class="sd">        n_features + 1, where the last item represents the intercept. For</span>
<span class="sd">        ``multiclass=&#39;multinomial&#39;``, the shape is (n_classes, n_cs,</span>
<span class="sd">        n_features) or (n_classes, n_cs, n_features + 1).</span>

<span class="sd">    Cs : ndarray</span>
<span class="sd">        Grid of Cs used for cross-validation.</span>

<span class="sd">    n_iter : array of shape (n_cs,)</span>
<span class="sd">        Actual number of iteration for each Cs.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    You might get slightly different results with the solver liblinear than</span>
<span class="sd">    with the others since this uses LIBLINEAR which penalizes the intercept.</span>

<span class="sd">    .. versionchanged:: 0.19</span>
<span class="sd">        The &quot;copy&quot; parameter was removed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">Cs</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">):</span>
        <span class="n">Cs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Cs</span><span class="p">)</span>

    <span class="n">solver</span> <span class="o">=</span> <span class="n">_check_solver</span><span class="p">(</span><span class="n">solver</span><span class="p">,</span> <span class="n">penalty</span><span class="p">,</span> <span class="n">dual</span><span class="p">)</span>

    <span class="c1"># Preprocessing.</span>
    <span class="k">if</span> <span class="n">check_input</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                        <span class="n">accept_large_sparse</span><span class="o">=</span><span class="n">solver</span> <span class="o">!=</span> <span class="s1">&#39;liblinear&#39;</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
        <span class="n">check_consistent_length</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="n">classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>

    <span class="n">multi_class</span> <span class="o">=</span> <span class="n">_check_multi_class</span><span class="p">(</span><span class="n">multi_class</span><span class="p">,</span> <span class="n">solver</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">pos_class</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">multi_class</span> <span class="o">!=</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">classes</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;To fit OvR, use the pos_class argument&#39;</span><span class="p">)</span>
        <span class="c1"># np.unique(y) gives labels in sorted order.</span>
        <span class="n">pos_class</span> <span class="o">=</span> <span class="n">classes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># If sample weights exist, convert them to array (support for lists)</span>
    <span class="c1"># and check length</span>
    <span class="c1"># Otherwise set them to 1 for all examples</span>
    <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">_check_sample_weight</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span>
                                         <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="c1"># If class_weights is a dict (provided by the user), the weights</span>
    <span class="c1"># are assigned to the original labels. If it is &quot;balanced&quot;, then</span>
    <span class="c1"># the class_weights are assigned after masking the labels with a OvR.</span>
    <span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">class_weight</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">or</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
        <span class="n">class_weight_</span> <span class="o">=</span> <span class="n">compute_class_weight</span><span class="p">(</span><span class="n">class_weight</span><span class="p">,</span>
                                             <span class="n">classes</span><span class="o">=</span><span class="n">classes</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
        <span class="n">sample_weight</span> <span class="o">*=</span> <span class="n">class_weight_</span><span class="p">[</span><span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)]</span>

    <span class="c1"># For doing a ovr, we need to mask the labels first. for the</span>
    <span class="c1"># multinomial case this is not necessary.</span>
    <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;ovr&#39;</span><span class="p">:</span>
        <span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_features</span> <span class="o">+</span> <span class="nb">int</span><span class="p">(</span><span class="n">fit_intercept</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">mask_classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">pos_class</span><span class="p">)</span>
        <span class="n">y_bin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">y_bin</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span>
        <span class="c1"># for compute_class_weight</span>

        <span class="k">if</span> <span class="n">class_weight</span> <span class="o">==</span> <span class="s2">&quot;balanced&quot;</span><span class="p">:</span>
            <span class="n">class_weight_</span> <span class="o">=</span> <span class="n">compute_class_weight</span><span class="p">(</span><span class="n">class_weight</span><span class="p">,</span>
                                                 <span class="n">classes</span><span class="o">=</span><span class="n">mask_classes</span><span class="p">,</span>
                                                 <span class="n">y</span><span class="o">=</span><span class="n">y_bin</span><span class="p">)</span>
            <span class="n">sample_weight</span> <span class="o">*=</span> <span class="n">class_weight_</span><span class="p">[</span><span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y_bin</span><span class="p">)]</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">solver</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;sag&#39;</span><span class="p">,</span> <span class="s1">&#39;saga&#39;</span><span class="p">]:</span>
            <span class="n">lbin</span> <span class="o">=</span> <span class="n">LabelBinarizer</span><span class="p">()</span>
            <span class="n">Y_multi</span> <span class="o">=</span> <span class="n">lbin</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">Y_multi</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">Y_multi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Y_multi</span><span class="p">,</span> <span class="n">Y_multi</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># SAG multinomial solver needs LabelEncoder, not LabelBinarizer</span>
            <span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
            <span class="n">Y_multi</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">classes</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">+</span> <span class="nb">int</span><span class="p">(</span><span class="n">fit_intercept</span><span class="p">)),</span>
                      <span class="n">order</span><span class="o">=</span><span class="s1">&#39;F&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">coef</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># it must work both giving the bias term and not</span>
        <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;ovr&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">coef</span><span class="o">.</span><span class="n">size</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">w0</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;Initialization coef is of shape </span><span class="si">%d</span><span class="s1">, expected shape &#39;</span>
                    <span class="s1">&#39;</span><span class="si">%d</span><span class="s1"> or </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">coef</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">w0</span><span class="o">.</span><span class="n">size</span><span class="p">))</span>
            <span class="n">w0</span><span class="p">[:</span><span class="n">coef</span><span class="o">.</span><span class="n">size</span><span class="p">]</span> <span class="o">=</span> <span class="n">coef</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># For binary problems coef.shape[0] should be 1, otherwise it</span>
            <span class="c1"># should be classes.size.</span>
            <span class="n">n_classes</span> <span class="o">=</span> <span class="n">classes</span><span class="o">.</span><span class="n">size</span>
            <span class="k">if</span> <span class="n">n_classes</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">n_classes</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="k">if</span> <span class="p">(</span><span class="n">coef</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">n_classes</span> <span class="ow">or</span>
                    <span class="n">coef</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;Initialization coef is of shape (</span><span class="si">%d</span><span class="s1">, </span><span class="si">%d</span><span class="s1">), expected &#39;</span>
                    <span class="s1">&#39;shape (</span><span class="si">%d</span><span class="s1">, </span><span class="si">%d</span><span class="s1">) or (</span><span class="si">%d</span><span class="s1">, </span><span class="si">%d</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span>
                        <span class="n">coef</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">coef</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">classes</span><span class="o">.</span><span class="n">size</span><span class="p">,</span>
                        <span class="n">n_features</span><span class="p">,</span> <span class="n">classes</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

            <span class="k">if</span> <span class="n">n_classes</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">w0</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="n">coef</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="o">-</span><span class="n">coef</span>
                <span class="n">w0</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:</span><span class="n">coef</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">coef</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">w0</span><span class="p">[:,</span> <span class="p">:</span><span class="n">coef</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">coef</span>

    <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
        <span class="c1"># scipy.optimize.minimize and newton-cg accepts only</span>
        <span class="c1"># ravelled parameters.</span>
        <span class="k">if</span> <span class="n">solver</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="s1">&#39;newton-cg&#39;</span><span class="p">]:</span>
            <span class="n">w0</span> <span class="o">=</span> <span class="n">w0</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">Y_multi</span>
        <span class="k">if</span> <span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;lbfgs&#39;</span><span class="p">:</span>
            <span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span> <span class="k">return</span> <span class="n">_multinomial_loss_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;newton-cg&#39;</span><span class="p">:</span>
            <span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span> <span class="k">return</span> <span class="n">_multinomial_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span> <span class="k">return</span> <span class="n">_multinomial_loss_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">hess</span> <span class="o">=</span> <span class="n">_multinomial_grad_hess</span>
        <span class="n">warm_start_sag</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;coef&#39;</span><span class="p">:</span> <span class="n">w0</span><span class="o">.</span><span class="n">T</span><span class="p">}</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">y_bin</span>
        <span class="k">if</span> <span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;lbfgs&#39;</span><span class="p">:</span>
            <span class="n">func</span> <span class="o">=</span> <span class="n">_logistic_loss_and_grad</span>
        <span class="k">elif</span> <span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;newton-cg&#39;</span><span class="p">:</span>
            <span class="n">func</span> <span class="o">=</span> <span class="n">_logistic_loss</span>
            <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span> <span class="k">return</span> <span class="n">_logistic_loss_and_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">hess</span> <span class="o">=</span> <span class="n">_logistic_grad_hess</span>
        <span class="n">warm_start_sag</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;coef&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)}</span>

    <span class="n">coefs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="n">n_iter</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Cs</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">C</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">Cs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;lbfgs&#39;</span><span class="p">:</span>
            <span class="n">iprint</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">101</span><span class="p">][</span>
                <span class="n">np</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">verbose</span><span class="p">)]</span>
            <span class="n">opt_res</span> <span class="o">=</span> <span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span>
                <span class="n">func</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;L-BFGS-B&quot;</span><span class="p">,</span> <span class="n">jac</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">C</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">),</span>
                <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;iprint&quot;</span><span class="p">:</span> <span class="n">iprint</span><span class="p">,</span> <span class="s2">&quot;gtol&quot;</span><span class="p">:</span> <span class="n">tol</span><span class="p">,</span> <span class="s2">&quot;maxiter&quot;</span><span class="p">:</span> <span class="n">max_iter</span><span class="p">}</span>
            <span class="p">)</span>
            <span class="n">n_iter_i</span> <span class="o">=</span> <span class="n">_check_optimize_result</span><span class="p">(</span>
                <span class="n">solver</span><span class="p">,</span> <span class="n">opt_res</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span>
                <span class="n">extra_warning_msg</span><span class="o">=</span><span class="n">_LOGISTIC_SOLVER_CONVERGENCE_MSG</span><span class="p">)</span>
            <span class="n">w0</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">opt_res</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">opt_res</span><span class="o">.</span><span class="n">fun</span>
        <span class="k">elif</span> <span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;newton-cg&#39;</span><span class="p">:</span>
            <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">C</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
            <span class="n">w0</span><span class="p">,</span> <span class="n">n_iter_i</span> <span class="o">=</span> <span class="n">_newton_cg</span><span class="p">(</span><span class="n">hess</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
                                      <span class="n">maxiter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;liblinear&#39;</span><span class="p">:</span>
            <span class="n">coef_</span><span class="p">,</span> <span class="n">intercept_</span><span class="p">,</span> <span class="n">n_iter_i</span><span class="p">,</span> <span class="o">=</span> <span class="n">_fit_liblinear</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="p">,</span> <span class="n">intercept_scaling</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
                <span class="n">penalty</span><span class="p">,</span> <span class="n">dual</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="p">,</span> <span class="n">random_state</span><span class="p">,</span>
                <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">fit_intercept</span><span class="p">:</span>
                <span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">coef_</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">intercept_</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">w0</span> <span class="o">=</span> <span class="n">coef_</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

        <span class="k">elif</span> <span class="n">solver</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;sag&#39;</span><span class="p">,</span> <span class="s1">&#39;saga&#39;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
                <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="s1">&#39;multinomial&#39;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="s1">&#39;log&#39;</span>
            <span class="c1"># alpha is for L2-norm, beta is for L1-norm</span>
            <span class="k">if</span> <span class="n">penalty</span> <span class="o">==</span> <span class="s1">&#39;l1&#39;</span><span class="p">:</span>
                <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.</span>
                <span class="n">beta</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">C</span>
            <span class="k">elif</span> <span class="n">penalty</span> <span class="o">==</span> <span class="s1">&#39;l2&#39;</span><span class="p">:</span>
                <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">C</span>
                <span class="n">beta</span> <span class="o">=</span> <span class="mf">0.</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># Elastic-Net penalty</span>
                <span class="n">alpha</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">C</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span>
                <span class="n">beta</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">C</span><span class="p">)</span> <span class="o">*</span> <span class="n">l1_ratio</span>

            <span class="n">w0</span><span class="p">,</span> <span class="n">n_iter_i</span><span class="p">,</span> <span class="n">warm_start_sag</span> <span class="o">=</span> <span class="n">sag_solver</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span>
                <span class="n">beta</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="p">,</span>
                <span class="n">verbose</span><span class="p">,</span> <span class="n">random_state</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">max_squared_sum</span><span class="p">,</span> <span class="n">warm_start_sag</span><span class="p">,</span>
                <span class="n">is_saga</span><span class="o">=</span><span class="p">(</span><span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;saga&#39;</span><span class="p">))</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;solver must be one of {&#39;liblinear&#39;, &#39;lbfgs&#39;, &quot;</span>
                             <span class="s2">&quot;&#39;newton-cg&#39;, &#39;sag&#39;}, got &#39;</span><span class="si">%s</span><span class="s2">&#39; instead&quot;</span> <span class="o">%</span> <span class="n">solver</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
            <span class="n">n_classes</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">classes</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
            <span class="n">multi_w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">n_classes</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">multi_w0</span> <span class="o">=</span> <span class="n">multi_w0</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">coefs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">multi_w0</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">coefs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w0</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>

        <span class="n">n_iter</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">n_iter_i</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">coefs</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Cs</span><span class="p">),</span> <span class="n">n_iter</span>


<span class="c1"># helper function for LogisticCV</span>
<span class="k">def</span> <span class="nf">_log_reg_scoring_path</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="n">pos_class</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">Cs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                          <span class="n">scoring</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                          <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span>
                          <span class="n">dual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">intercept_scaling</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
                          <span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">max_squared_sum</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">l1_ratio</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes scores across logistic_regression_path</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        Training data.</span>

<span class="sd">    y : array-like of shape (n_samples,) or (n_samples, n_targets)</span>
<span class="sd">        Target labels.</span>

<span class="sd">    train : list of indices</span>
<span class="sd">        The indices of the train set.</span>

<span class="sd">    test : list of indices</span>
<span class="sd">        The indices of the test set.</span>

<span class="sd">    pos_class : int, default=None</span>
<span class="sd">        The class with respect to which we perform a one-vs-all fit.</span>
<span class="sd">        If None, then it is assumed that the given problem is binary.</span>

<span class="sd">    Cs : int or list of floats, default=10</span>
<span class="sd">        Each of the values in Cs describes the inverse of</span>
<span class="sd">        regularization strength. If Cs is as an int, then a grid of Cs</span>
<span class="sd">        values are chosen in a logarithmic scale between 1e-4 and 1e4.</span>
<span class="sd">        If not provided, then a fixed set of values for Cs are used.</span>

<span class="sd">    scoring : callable, default=None</span>
<span class="sd">        A string (see model evaluation documentation) or</span>
<span class="sd">        a scorer callable object / function with signature</span>
<span class="sd">        ``scorer(estimator, X, y)``. For a list of scoring functions</span>
<span class="sd">        that can be used, look at :mod:`sklearn.metrics`. The</span>
<span class="sd">        default scoring option used is accuracy_score.</span>

<span class="sd">    fit_intercept : bool, default=False</span>
<span class="sd">        If False, then the bias term is set to zero. Else the last</span>
<span class="sd">        term of each coef_ gives us the intercept.</span>

<span class="sd">    max_iter : int, default=100</span>
<span class="sd">        Maximum number of iterations for the solver.</span>

<span class="sd">    tol : float, default=1e-4</span>
<span class="sd">        Tolerance for stopping criteria.</span>

<span class="sd">    class_weight : dict or &#39;balanced&#39;, default=None</span>
<span class="sd">        Weights associated with classes in the form ``{class_label: weight}``.</span>
<span class="sd">        If not given, all classes are supposed to have weight one.</span>

<span class="sd">        The &quot;balanced&quot; mode uses the values of y to automatically adjust</span>
<span class="sd">        weights inversely proportional to class frequencies in the input data</span>
<span class="sd">        as ``n_samples / (n_classes * np.bincount(y))``</span>

<span class="sd">        Note that these weights will be multiplied with sample_weight (passed</span>
<span class="sd">        through the fit method) if sample_weight is specified.</span>

<span class="sd">    verbose : int, default=0</span>
<span class="sd">        For the liblinear and lbfgs solvers set verbose to any positive</span>
<span class="sd">        number for verbosity.</span>

<span class="sd">    solver : {&#39;lbfgs&#39;, &#39;newton-cg&#39;, &#39;liblinear&#39;, &#39;sag&#39;, &#39;saga&#39;}, \</span>
<span class="sd">            default=&#39;lbfgs&#39;</span>
<span class="sd">        Decides which solver to use.</span>

<span class="sd">    penalty : {&#39;l1&#39;, &#39;l2&#39;, &#39;elasticnet&#39;}, default=&#39;l2&#39;</span>
<span class="sd">        Used to specify the norm used in the penalization. The &#39;newton-cg&#39;,</span>
<span class="sd">        &#39;sag&#39; and &#39;lbfgs&#39; solvers support only l2 penalties. &#39;elasticnet&#39; is</span>
<span class="sd">        only supported by the &#39;saga&#39; solver.</span>

<span class="sd">    dual : bool, default=False</span>
<span class="sd">        Dual or primal formulation. Dual formulation is only implemented for</span>
<span class="sd">        l2 penalty with liblinear solver. Prefer dual=False when</span>
<span class="sd">        n_samples &gt; n_features.</span>

<span class="sd">    intercept_scaling : float, default=1.</span>
<span class="sd">        Useful only when the solver &#39;liblinear&#39; is used</span>
<span class="sd">        and self.fit_intercept is set to True. In this case, x becomes</span>
<span class="sd">        [x, self.intercept_scaling],</span>
<span class="sd">        i.e. a &quot;synthetic&quot; feature with constant value equals to</span>
<span class="sd">        intercept_scaling is appended to the instance vector.</span>
<span class="sd">        The intercept becomes intercept_scaling * synthetic feature weight</span>
<span class="sd">        Note! the synthetic feature weight is subject to l1/l2 regularization</span>
<span class="sd">        as all other features.</span>
<span class="sd">        To lessen the effect of regularization on synthetic feature weight</span>
<span class="sd">        (and therefore on the intercept) intercept_scaling has to be increased.</span>

<span class="sd">    multi_class : {&#39;auto&#39;, &#39;ovr&#39;, &#39;multinomial&#39;}, default=&#39;auto&#39;</span>
<span class="sd">        If the option chosen is &#39;ovr&#39;, then a binary problem is fit for each</span>
<span class="sd">        label. For &#39;multinomial&#39; the loss minimised is the multinomial loss fit</span>
<span class="sd">        across the entire probability distribution, *even when the data is</span>
<span class="sd">        binary*. &#39;multinomial&#39; is unavailable when solver=&#39;liblinear&#39;.</span>

<span class="sd">    random_state : int, RandomState instance, default=None</span>
<span class="sd">        Used when ``solver`` == &#39;sag&#39;, &#39;saga&#39; or &#39;liblinear&#39; to shuffle the</span>
<span class="sd">        data. See :term:`Glossary &lt;random_state&gt;` for details.</span>

<span class="sd">    max_squared_sum : float, default=None</span>
<span class="sd">        Maximum squared sum of X over samples. Used only in SAG solver.</span>
<span class="sd">        If None, it will be computed, going through all the samples.</span>
<span class="sd">        The value should be precomputed to speed up cross validation.</span>

<span class="sd">    sample_weight : array-like of shape(n_samples,), default=None</span>
<span class="sd">        Array of weights that are assigned to individual samples.</span>
<span class="sd">        If not provided, then each sample is given unit weight.</span>

<span class="sd">    l1_ratio : float, default=None</span>
<span class="sd">        The Elastic-Net mixing parameter, with ``0 &lt;= l1_ratio &lt;= 1``. Only</span>
<span class="sd">        used if ``penalty=&#39;elasticnet&#39;``. Setting ``l1_ratio=0`` is equivalent</span>
<span class="sd">        to using ``penalty=&#39;l2&#39;``, while setting ``l1_ratio=1`` is equivalent</span>
<span class="sd">        to using ``penalty=&#39;l1&#39;``. For ``0 &lt; l1_ratio &lt;1``, the penalty is a</span>
<span class="sd">        combination of L1 and L2.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    coefs : ndarray of shape (n_cs, n_features) or (n_cs, n_features + 1)</span>
<span class="sd">        List of coefficients for the Logistic Regression model. If</span>
<span class="sd">        fit_intercept is set to True then the second dimension will be</span>
<span class="sd">        n_features + 1, where the last item represents the intercept.</span>

<span class="sd">    Cs : ndarray</span>
<span class="sd">        Grid of Cs used for cross-validation.</span>

<span class="sd">    scores : ndarray of shape (n_cs,)</span>
<span class="sd">        Scores obtained for each Cs.</span>

<span class="sd">    n_iter : ndarray of shape(n_cs,)</span>
<span class="sd">        Actual number of iteration for each Cs.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">]</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">]</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">]</span>
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">_check_sample_weight</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="p">[</span><span class="n">train</span><span class="p">]</span>

    <span class="n">coefs</span><span class="p">,</span> <span class="n">Cs</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="n">_logistic_regression_path</span><span class="p">(</span>
        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">Cs</span><span class="o">=</span><span class="n">Cs</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
        <span class="n">fit_intercept</span><span class="o">=</span><span class="n">fit_intercept</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="n">solver</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
        <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weight</span><span class="p">,</span> <span class="n">pos_class</span><span class="o">=</span><span class="n">pos_class</span><span class="p">,</span>
        <span class="n">multi_class</span><span class="o">=</span><span class="n">multi_class</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span> <span class="n">dual</span><span class="o">=</span><span class="n">dual</span><span class="p">,</span>
        <span class="n">penalty</span><span class="o">=</span><span class="n">penalty</span><span class="p">,</span> <span class="n">intercept_scaling</span><span class="o">=</span><span class="n">intercept_scaling</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">max_squared_sum</span><span class="o">=</span><span class="n">max_squared_sum</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>

    <span class="n">log_reg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="n">solver</span><span class="p">,</span> <span class="n">multi_class</span><span class="o">=</span><span class="n">multi_class</span><span class="p">)</span>

    <span class="c1"># The score method of Logistic Regression has a classes_ attribute.</span>
    <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;ovr&#39;</span><span class="p">:</span>
        <span class="n">log_reg</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="k">elif</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
        <span class="n">log_reg</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;multi_class should be either multinomial or ovr, &quot;</span>
                         <span class="s2">&quot;got </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">multi_class</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">pos_class</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_test</span> <span class="o">==</span> <span class="n">pos_class</span><span class="p">)</span>
        <span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">y_test</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span>

    <span class="n">scores</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

    <span class="n">scoring</span> <span class="o">=</span> <span class="n">get_scorer</span><span class="p">(</span><span class="n">scoring</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">coefs</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;ovr&#39;</span><span class="p">:</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">if</span> <span class="n">fit_intercept</span><span class="p">:</span>
            <span class="n">log_reg</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">w</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">log_reg</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">w</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">log_reg</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">w</span>
            <span class="n">log_reg</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="mf">0.</span>

        <span class="k">if</span> <span class="n">scoring</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scoring</span><span class="p">(</span><span class="n">log_reg</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">coefs</span><span class="p">,</span> <span class="n">Cs</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">n_iter</span>


<span class="k">class</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">LinearClassifierMixin</span><span class="p">,</span>
                         <span class="n">SparseCoefMixin</span><span class="p">,</span>
                         <span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Logistic Regression (aka logit, MaxEnt) classifier.</span>

<span class="sd">    In the multiclass case, the training algorithm uses the one-vs-rest (OvR)</span>
<span class="sd">    scheme if the &#39;multi_class&#39; option is set to &#39;ovr&#39;, and uses the</span>
<span class="sd">    cross-entropy loss if the &#39;multi_class&#39; option is set to &#39;multinomial&#39;.</span>
<span class="sd">    (Currently the &#39;multinomial&#39; option is supported only by the &#39;lbfgs&#39;,</span>
<span class="sd">    &#39;sag&#39;, &#39;saga&#39; and &#39;newton-cg&#39; solvers.)</span>

<span class="sd">    This class implements regularized logistic regression using the</span>
<span class="sd">    &#39;liblinear&#39; library, &#39;newton-cg&#39;, &#39;sag&#39;, &#39;saga&#39; and &#39;lbfgs&#39; solvers. **Note</span>
<span class="sd">    that regularization is applied by default**. It can handle both dense</span>
<span class="sd">    and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit</span>
<span class="sd">    floats for optimal performance; any other input format will be converted</span>
<span class="sd">    (and copied).</span>

<span class="sd">    The &#39;newton-cg&#39;, &#39;sag&#39;, and &#39;lbfgs&#39; solvers support only L2 regularization</span>
<span class="sd">    with primal formulation, or no regularization. The &#39;liblinear&#39; solver</span>
<span class="sd">    supports both L1 and L2 regularization, with a dual formulation only for</span>
<span class="sd">    the L2 penalty. The Elastic-Net regularization is only supported by the</span>
<span class="sd">    &#39;saga&#39; solver.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;logistic_regression&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    penalty : {&#39;l1&#39;, &#39;l2&#39;, &#39;elasticnet&#39;, &#39;none&#39;}, default=&#39;l2&#39;</span>
<span class="sd">        Used to specify the norm used in the penalization. The &#39;newton-cg&#39;,</span>
<span class="sd">        &#39;sag&#39; and &#39;lbfgs&#39; solvers support only l2 penalties. &#39;elasticnet&#39; is</span>
<span class="sd">        only supported by the &#39;saga&#39; solver. If &#39;none&#39; (not supported by the</span>
<span class="sd">        liblinear solver), no regularization is applied.</span>

<span class="sd">        .. versionadded:: 0.19</span>
<span class="sd">           l1 penalty with SAGA solver (allowing &#39;multinomial&#39; + L1)</span>

<span class="sd">    dual : bool, default=False</span>
<span class="sd">        Dual or primal formulation. Dual formulation is only implemented for</span>
<span class="sd">        l2 penalty with liblinear solver. Prefer dual=False when</span>
<span class="sd">        n_samples &gt; n_features.</span>

<span class="sd">    tol : float, default=1e-4</span>
<span class="sd">        Tolerance for stopping criteria.</span>

<span class="sd">    C : float, default=1.0</span>
<span class="sd">        Inverse of regularization strength; must be a positive float.</span>
<span class="sd">        Like in support vector machines, smaller values specify stronger</span>
<span class="sd">        regularization.</span>

<span class="sd">    fit_intercept : bool, default=True</span>
<span class="sd">        Specifies if a constant (a.k.a. bias or intercept) should be</span>
<span class="sd">        added to the decision function.</span>

<span class="sd">    intercept_scaling : float, default=1</span>
<span class="sd">        Useful only when the solver &#39;liblinear&#39; is used</span>
<span class="sd">        and self.fit_intercept is set to True. In this case, x becomes</span>
<span class="sd">        [x, self.intercept_scaling],</span>
<span class="sd">        i.e. a &quot;synthetic&quot; feature with constant value equal to</span>
<span class="sd">        intercept_scaling is appended to the instance vector.</span>
<span class="sd">        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.</span>

<span class="sd">        Note! the synthetic feature weight is subject to l1/l2 regularization</span>
<span class="sd">        as all other features.</span>
<span class="sd">        To lessen the effect of regularization on synthetic feature weight</span>
<span class="sd">        (and therefore on the intercept) intercept_scaling has to be increased.</span>

<span class="sd">    class_weight : dict or &#39;balanced&#39;, default=None</span>
<span class="sd">        Weights associated with classes in the form ``{class_label: weight}``.</span>
<span class="sd">        If not given, all classes are supposed to have weight one.</span>

<span class="sd">        The &quot;balanced&quot; mode uses the values of y to automatically adjust</span>
<span class="sd">        weights inversely proportional to class frequencies in the input data</span>
<span class="sd">        as ``n_samples / (n_classes * np.bincount(y))``.</span>

<span class="sd">        Note that these weights will be multiplied with sample_weight (passed</span>
<span class="sd">        through the fit method) if sample_weight is specified.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           *class_weight=&#39;balanced&#39;*</span>

<span class="sd">    random_state : int, RandomState instance, default=None</span>
<span class="sd">        Used when ``solver`` == &#39;sag&#39;, &#39;saga&#39; or &#39;liblinear&#39; to shuffle the</span>
<span class="sd">        data. See :term:`Glossary &lt;random_state&gt;` for details.</span>

<span class="sd">    solver : {&#39;newton-cg&#39;, &#39;lbfgs&#39;, &#39;liblinear&#39;, &#39;sag&#39;, &#39;saga&#39;}, \</span>
<span class="sd">            default=&#39;lbfgs&#39;</span>

<span class="sd">        Algorithm to use in the optimization problem.</span>

<span class="sd">        - For small datasets, &#39;liblinear&#39; is a good choice, whereas &#39;sag&#39; and</span>
<span class="sd">          &#39;saga&#39; are faster for large ones.</span>
<span class="sd">        - For multiclass problems, only &#39;newton-cg&#39;, &#39;sag&#39;, &#39;saga&#39; and &#39;lbfgs&#39;</span>
<span class="sd">          handle multinomial loss; &#39;liblinear&#39; is limited to one-versus-rest</span>
<span class="sd">          schemes.</span>
<span class="sd">        - &#39;newton-cg&#39;, &#39;lbfgs&#39;, &#39;sag&#39; and &#39;saga&#39; handle L2 or no penalty</span>
<span class="sd">        - &#39;liblinear&#39; and &#39;saga&#39; also handle L1 penalty</span>
<span class="sd">        - &#39;saga&#39; also supports &#39;elasticnet&#39; penalty</span>
<span class="sd">        - &#39;liblinear&#39; does not support setting ``penalty=&#39;none&#39;``</span>

<span class="sd">        Note that &#39;sag&#39; and &#39;saga&#39; fast convergence is only guaranteed on</span>
<span class="sd">        features with approximately the same scale. You can</span>
<span class="sd">        preprocess the data with a scaler from sklearn.preprocessing.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           Stochastic Average Gradient descent solver.</span>
<span class="sd">        .. versionadded:: 0.19</span>
<span class="sd">           SAGA solver.</span>
<span class="sd">        .. versionchanged:: 0.22</span>
<span class="sd">            The default solver changed from &#39;liblinear&#39; to &#39;lbfgs&#39; in 0.22.</span>

<span class="sd">    max_iter : int, default=100</span>
<span class="sd">        Maximum number of iterations taken for the solvers to converge.</span>

<span class="sd">    multi_class : {&#39;auto&#39;, &#39;ovr&#39;, &#39;multinomial&#39;}, default=&#39;auto&#39;</span>
<span class="sd">        If the option chosen is &#39;ovr&#39;, then a binary problem is fit for each</span>
<span class="sd">        label. For &#39;multinomial&#39; the loss minimised is the multinomial loss fit</span>
<span class="sd">        across the entire probability distribution, *even when the data is</span>
<span class="sd">        binary*. &#39;multinomial&#39; is unavailable when solver=&#39;liblinear&#39;.</span>
<span class="sd">        &#39;auto&#39; selects &#39;ovr&#39; if the data is binary, or if solver=&#39;liblinear&#39;,</span>
<span class="sd">        and otherwise selects &#39;multinomial&#39;.</span>

<span class="sd">        .. versionadded:: 0.18</span>
<span class="sd">           Stochastic Average Gradient descent solver for &#39;multinomial&#39; case.</span>
<span class="sd">        .. versionchanged:: 0.22</span>
<span class="sd">            Default changed from &#39;ovr&#39; to &#39;auto&#39; in 0.22.</span>

<span class="sd">    verbose : int, default=0</span>
<span class="sd">        For the liblinear and lbfgs solvers set verbose to any positive</span>
<span class="sd">        number for verbosity.</span>

<span class="sd">    warm_start : bool, default=False</span>
<span class="sd">        When set to True, reuse the solution of the previous call to fit as</span>
<span class="sd">        initialization, otherwise, just erase the previous solution.</span>
<span class="sd">        Useless for liblinear solver. See :term:`the Glossary &lt;warm_start&gt;`.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.</span>

<span class="sd">    n_jobs : int, default=None</span>
<span class="sd">        Number of CPU cores used when parallelizing over classes if</span>
<span class="sd">        multi_class=&#39;ovr&#39;&quot;. This parameter is ignored when the ``solver`` is</span>
<span class="sd">        set to &#39;liblinear&#39; regardless of whether &#39;multi_class&#39; is specified or</span>
<span class="sd">        not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`</span>
<span class="sd">        context. ``-1`` means using all processors.</span>
<span class="sd">        See :term:`Glossary &lt;n_jobs&gt;` for more details.</span>

<span class="sd">    l1_ratio : float, default=None</span>
<span class="sd">        The Elastic-Net mixing parameter, with ``0 &lt;= l1_ratio &lt;= 1``. Only</span>
<span class="sd">        used if ``penalty=&#39;elasticnet&#39;``. Setting ``l1_ratio=0`` is equivalent</span>
<span class="sd">        to using ``penalty=&#39;l2&#39;``, while setting ``l1_ratio=1`` is equivalent</span>
<span class="sd">        to using ``penalty=&#39;l1&#39;``. For ``0 &lt; l1_ratio &lt;1``, the penalty is a</span>
<span class="sd">        combination of L1 and L2.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>

<span class="sd">    classes_ : ndarray of shape (n_classes, )</span>
<span class="sd">        A list of class labels known to the classifier.</span>

<span class="sd">    coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)</span>
<span class="sd">        Coefficient of the features in the decision function.</span>

<span class="sd">        `coef_` is of shape (1, n_features) when the given problem is binary.</span>
<span class="sd">        In particular, when `multi_class=&#39;multinomial&#39;`, `coef_` corresponds</span>
<span class="sd">        to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).</span>

<span class="sd">    intercept_ : ndarray of shape (1,) or (n_classes,)</span>
<span class="sd">        Intercept (a.k.a. bias) added to the decision function.</span>

<span class="sd">        If `fit_intercept` is set to False, the intercept is set to zero.</span>
<span class="sd">        `intercept_` is of shape (1,) when the given problem is binary.</span>
<span class="sd">        In particular, when `multi_class=&#39;multinomial&#39;`, `intercept_`</span>
<span class="sd">        corresponds to outcome 1 (True) and `-intercept_` corresponds to</span>
<span class="sd">        outcome 0 (False).</span>

<span class="sd">    n_iter_ : ndarray of shape (n_classes,) or (1, )</span>
<span class="sd">        Actual number of iterations for all classes. If binary or multinomial,</span>
<span class="sd">        it returns only 1 element. For liblinear solver, only the maximum</span>
<span class="sd">        number of iteration across all classes is given.</span>

<span class="sd">        .. versionchanged:: 0.20</span>

<span class="sd">            In SciPy &lt;= 1.0.0 the number of lbfgs iterations may exceed</span>
<span class="sd">            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    SGDClassifier : Incrementally trained logistic regression (when given</span>
<span class="sd">        the parameter ``loss=&quot;log&quot;``).</span>
<span class="sd">    LogisticRegressionCV : Logistic regression with built-in cross validation.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The underlying C implementation uses a random number generator to</span>
<span class="sd">    select features when fitting the model. It is thus not uncommon,</span>
<span class="sd">    to have slightly different results for the same input data. If</span>
<span class="sd">    that happens, try with a smaller tol parameter.</span>

<span class="sd">    Predict output may not match that of standalone liblinear in certain</span>
<span class="sd">    cases. See :ref:`differences from liblinear &lt;liblinear_differences&gt;`</span>
<span class="sd">    in the narrative documentation.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>

<span class="sd">    L-BFGS-B -- Software for Large-scale Bound-constrained Optimization</span>
<span class="sd">        Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.</span>
<span class="sd">        http://users.iems.northwestern.edu/~nocedal/lbfgsb.html</span>

<span class="sd">    LIBLINEAR -- A Library for Large Linear Classification</span>
<span class="sd">        https://www.csie.ntu.edu.tw/~cjlin/liblinear/</span>

<span class="sd">    SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach</span>
<span class="sd">        Minimizing Finite Sums with the Stochastic Average Gradient</span>
<span class="sd">        https://hal.inria.fr/hal-00860051/document</span>

<span class="sd">    SAGA -- Defazio, A., Bach F. &amp; Lacoste-Julien S. (2014).</span>
<span class="sd">        SAGA: A Fast Incremental Gradient Method With Support</span>
<span class="sd">        for Non-Strongly Convex Composite Objectives</span>
<span class="sd">        https://arxiv.org/abs/1407.0202</span>

<span class="sd">    Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent</span>
<span class="sd">        methods for logistic regression and maximum entropy models.</span>
<span class="sd">        Machine Learning 85(1-2):41-75.</span>
<span class="sd">        https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.datasets import load_iris</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.linear_model import LogisticRegression</span>
<span class="sd">    &gt;&gt;&gt; X, y = load_iris(return_X_y=True)</span>
<span class="sd">    &gt;&gt;&gt; clf = LogisticRegression(random_state=0).fit(X, y)</span>
<span class="sd">    &gt;&gt;&gt; clf.predict(X[:2, :])</span>
<span class="sd">    array([0, 0])</span>
<span class="sd">    &gt;&gt;&gt; clf.predict_proba(X[:2, :])</span>
<span class="sd">    array([[9.8...e-01, 1.8...e-02, 1.4...e-08],</span>
<span class="sd">           [9.7...e-01, 2.8...e-02, ...e-08]])</span>
<span class="sd">    &gt;&gt;&gt; clf.score(X, y)</span>
<span class="sd">    0.97...</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">dual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">intercept_scaling</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">l1_ratio</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span> <span class="o">=</span> <span class="n">penalty</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dual</span> <span class="o">=</span> <span class="n">dual</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">C</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">fit_intercept</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intercept_scaling</span> <span class="o">=</span> <span class="n">intercept_scaling</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="o">=</span> <span class="n">class_weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">solver</span> <span class="o">=</span> <span class="n">solver</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span> <span class="o">=</span> <span class="n">multi_class</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="o">=</span> <span class="n">warm_start</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span> <span class="o">=</span> <span class="n">l1_ratio</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fit the model according to the given training data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Training vector, where n_samples is the number of samples and</span>
<span class="sd">            n_features is the number of features.</span>

<span class="sd">        y : array-like of shape (n_samples,)</span>
<span class="sd">            Target vector relative to X.</span>

<span class="sd">        sample_weight : array-like of shape (n_samples,) default=None</span>
<span class="sd">            Array of weights that are assigned to individual samples.</span>
<span class="sd">            If not provided, then each sample is given unit weight.</span>

<span class="sd">            .. versionadded:: 0.17</span>
<span class="sd">               *sample_weight* support to LogisticRegression.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self</span>
<span class="sd">            Fitted estimator.</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        The SAGA solver supports both float64 and float32 bit arrays.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">solver</span> <span class="o">=</span> <span class="n">_check_solver</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">solver</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dual</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Penalty term must be positive; got (C=</span><span class="si">%r</span><span class="s2">)&quot;</span>
                             <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span> <span class="o">==</span> <span class="s1">&#39;elasticnet&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)</span> <span class="ow">or</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;l1_ratio must be between 0 and 1;&quot;</span>
                                 <span class="s2">&quot; got (l1_ratio=</span><span class="si">%r</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;l1_ratio parameter is only used when penalty is &quot;</span>
                          <span class="s2">&quot;&#39;elasticnet&#39;. Got &quot;</span>
                          <span class="s2">&quot;(penalty=</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span> <span class="o">==</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>  <span class="c1"># default values</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;Setting penalty=&#39;none&#39; will ignore the C and l1_ratio &quot;</span>
                    <span class="s2">&quot;parameters&quot;</span>
                <span class="p">)</span>
                <span class="c1"># Note that check for l1_ratio is done right above</span>
            <span class="n">C_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
            <span class="n">penalty</span> <span class="o">=</span> <span class="s1">&#39;l2&#39;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">C_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span>
            <span class="n">penalty</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Maximum number of iteration must be positive;&quot;</span>
                             <span class="s2">&quot; got (max_iter=</span><span class="si">%r</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Tolerance for stopping criteria must be &quot;</span>
                             <span class="s2">&quot;positive; got (tol=</span><span class="si">%r</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;lbfgs&#39;</span><span class="p">:</span>
            <span class="n">_dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_dtype</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>

        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">_dtype</span><span class="p">,</span>
                                   <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span>
                                   <span class="n">accept_large_sparse</span><span class="o">=</span><span class="n">solver</span> <span class="o">!=</span> <span class="s1">&#39;liblinear&#39;</span><span class="p">)</span>
        <span class="n">check_classification_targets</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="n">multi_class</span> <span class="o">=</span> <span class="n">_check_multi_class</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span><span class="p">,</span> <span class="n">solver</span><span class="p">,</span>
                                         <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;liblinear&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">effective_n_jobs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;&#39;n_jobs&#39; &gt; 1 does not have any effect when&quot;</span>
                              <span class="s2">&quot; &#39;solver&#39; is set to &#39;liblinear&#39;. Got &#39;n_jobs&#39;&quot;</span>
                              <span class="s2">&quot; = </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">effective_n_jobs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="n">n_iter_</span> <span class="o">=</span> <span class="n">_fit_liblinear</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_scaling</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dual</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
                <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">n_iter_</span><span class="p">])</span>
            <span class="k">return</span> <span class="bp">self</span>

        <span class="k">if</span> <span class="n">solver</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;sag&#39;</span><span class="p">,</span> <span class="s1">&#39;saga&#39;</span><span class="p">]:</span>
            <span class="n">max_squared_sum</span> <span class="o">=</span> <span class="n">row_norms</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">max_squared_sum</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">n_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
        <span class="n">classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span>
        <span class="k">if</span> <span class="n">n_classes</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;This solver needs samples of at least 2 classes&quot;</span>
                             <span class="s2">&quot; in the data, but the data contains only one&quot;</span>
                             <span class="s2">&quot; class: </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">classes_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">n_classes</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">classes_</span> <span class="o">=</span> <span class="n">classes_</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span><span class="p">:</span>
            <span class="n">warm_start_coef</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;coef_&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">warm_start_coef</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">warm_start_coef</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">:</span>
            <span class="n">warm_start_coef</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">warm_start_coef</span><span class="p">,</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span>
                                        <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Hack so that we iterate only once for the multinomial case.</span>
        <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
            <span class="n">classes_</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span>
            <span class="n">warm_start_coef</span> <span class="o">=</span> <span class="p">[</span><span class="n">warm_start_coef</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">warm_start_coef</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warm_start_coef</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">n_classes</span>

        <span class="n">path_func</span> <span class="o">=</span> <span class="n">delayed</span><span class="p">(</span><span class="n">_logistic_regression_path</span><span class="p">)</span>

        <span class="c1"># The SAG solver releases the GIL so it&#39;s more efficient to use</span>
        <span class="c1"># threads for this solver.</span>
        <span class="k">if</span> <span class="n">solver</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;sag&#39;</span><span class="p">,</span> <span class="s1">&#39;saga&#39;</span><span class="p">]:</span>
            <span class="n">prefer</span> <span class="o">=</span> <span class="s1">&#39;threads&#39;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">prefer</span> <span class="o">=</span> <span class="s1">&#39;processes&#39;</span>
        <span class="n">fold_coefs_</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                               <span class="o">**</span><span class="n">_joblib_parallel_args</span><span class="p">(</span><span class="n">prefer</span><span class="o">=</span><span class="n">prefer</span><span class="p">))(</span>
            <span class="n">path_func</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pos_class</span><span class="o">=</span><span class="n">class_</span><span class="p">,</span> <span class="n">Cs</span><span class="o">=</span><span class="p">[</span><span class="n">C_</span><span class="p">],</span>
                      <span class="n">l1_ratio</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">,</span>
                      <span class="n">tol</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="n">solver</span><span class="p">,</span>
                      <span class="n">multi_class</span><span class="o">=</span><span class="n">multi_class</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span>
                      <span class="n">class_weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                      <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span> <span class="n">coef</span><span class="o">=</span><span class="n">warm_start_coef_</span><span class="p">,</span>
                      <span class="n">penalty</span><span class="o">=</span><span class="n">penalty</span><span class="p">,</span> <span class="n">max_squared_sum</span><span class="o">=</span><span class="n">max_squared_sum</span><span class="p">,</span>
                      <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">class_</span><span class="p">,</span> <span class="n">warm_start_coef_</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">classes_</span><span class="p">,</span> <span class="n">warm_start_coef</span><span class="p">))</span>

        <span class="n">fold_coefs_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">n_iter_</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">fold_coefs_</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">n_iter_</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>

        <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">fold_coefs_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">fold_coefs_</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">+</span>
                                            <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Probability estimates.</span>

<span class="sd">        The returned estimates for all classes are ordered by the</span>
<span class="sd">        label of classes.</span>

<span class="sd">        For a multi_class problem, if multi_class is set to be &quot;multinomial&quot;</span>
<span class="sd">        the softmax function is used to find the predicted probability of</span>
<span class="sd">        each class.</span>
<span class="sd">        Else use a one-vs-rest approach, i.e calculate the probability</span>
<span class="sd">        of each class assuming it to be positive using the logistic function.</span>
<span class="sd">        and normalize these values across all the classes.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like of shape (n_samples, n_features)</span>
<span class="sd">            Vector to be scored, where `n_samples` is the number of samples and</span>
<span class="sd">            `n_features` is the number of features.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        T : array-like of shape (n_samples, n_classes)</span>
<span class="sd">            Returns the probability of the sample for each class in the model,</span>
<span class="sd">            where classes are ordered as they are in ``self.classes_``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="n">ovr</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;ovr&quot;</span><span class="p">,</span> <span class="s2">&quot;warn&quot;</span><span class="p">]</span> <span class="ow">or</span>
               <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;auto&#39;</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">size</span> <span class="o">&lt;=</span> <span class="mi">2</span> <span class="ow">or</span>
                                                <span class="bp">self</span><span class="o">.</span><span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;liblinear&#39;</span><span class="p">)))</span>
        <span class="k">if</span> <span class="n">ovr</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_predict_proba_lr</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">decision</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">decision</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># Workaround for multi_class=&quot;multinomial&quot; and binary outcomes</span>
                <span class="c1"># which requires softmax prediction with only a 1D decision.</span>
                <span class="n">decision_2d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="o">-</span><span class="n">decision</span><span class="p">,</span> <span class="n">decision</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">decision_2d</span> <span class="o">=</span> <span class="n">decision</span>
            <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="n">decision_2d</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict_log_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Predict logarithm of probability estimates.</span>

<span class="sd">        The returned estimates for all classes are ordered by the</span>
<span class="sd">        label of classes.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like of shape (n_samples, n_features)</span>
<span class="sd">            Vector to be scored, where `n_samples` is the number of samples and</span>
<span class="sd">            `n_features` is the number of features.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        T : array-like of shape (n_samples, n_classes)</span>
<span class="sd">            Returns the log-probability of the sample for each class in the</span>
<span class="sd">            model, where classes are ordered as they are in ``self.classes_``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">LogisticRegressionCV</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">,</span>
                           <span class="n">LinearClassifierMixin</span><span class="p">,</span>
                           <span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Logistic Regression CV (aka logit, MaxEnt) classifier.</span>

<span class="sd">    See glossary entry for :term:`cross-validation estimator`.</span>

<span class="sd">    This class implements logistic regression using liblinear, newton-cg, sag</span>
<span class="sd">    of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2</span>
<span class="sd">    regularization with primal formulation. The liblinear solver supports both</span>
<span class="sd">    L1 and L2 regularization, with a dual formulation only for the L2 penalty.</span>
<span class="sd">    Elastic-Net penalty is only supported by the saga solver.</span>

<span class="sd">    For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter</span>
<span class="sd">    is selected by the cross-validator</span>
<span class="sd">    :class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed</span>
<span class="sd">    using the :term:`cv` parameter. The &#39;newton-cg&#39;, &#39;sag&#39;, &#39;saga&#39; and &#39;lbfgs&#39;</span>
<span class="sd">    solvers can warm-start the coefficients (see :term:`Glossary&lt;warm_start&gt;`).</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;logistic_regression&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    Cs : int or list of floats, default=10</span>
<span class="sd">        Each of the values in Cs describes the inverse of regularization</span>
<span class="sd">        strength. If Cs is as an int, then a grid of Cs values are chosen</span>
<span class="sd">        in a logarithmic scale between 1e-4 and 1e4.</span>
<span class="sd">        Like in support vector machines, smaller values specify stronger</span>
<span class="sd">        regularization.</span>

<span class="sd">    fit_intercept : bool, default=True</span>
<span class="sd">        Specifies if a constant (a.k.a. bias or intercept) should be</span>
<span class="sd">        added to the decision function.</span>

<span class="sd">    cv : int or cross-validation generator, default=None</span>
<span class="sd">        The default cross-validation generator used is Stratified K-Folds.</span>
<span class="sd">        If an integer is provided, then it is the number of folds used.</span>
<span class="sd">        See the module :mod:`sklearn.model_selection` module for the</span>
<span class="sd">        list of possible cross-validation objects.</span>

<span class="sd">        .. versionchanged:: 0.22</span>
<span class="sd">            ``cv`` default value if None changed from 3-fold to 5-fold.</span>

<span class="sd">    dual : bool, default=False</span>
<span class="sd">        Dual or primal formulation. Dual formulation is only implemented for</span>
<span class="sd">        l2 penalty with liblinear solver. Prefer dual=False when</span>
<span class="sd">        n_samples &gt; n_features.</span>

<span class="sd">    penalty : {&#39;l1&#39;, &#39;l2&#39;, &#39;elasticnet&#39;}, default=&#39;l2&#39;</span>
<span class="sd">        Used to specify the norm used in the penalization. The &#39;newton-cg&#39;,</span>
<span class="sd">        &#39;sag&#39; and &#39;lbfgs&#39; solvers support only l2 penalties. &#39;elasticnet&#39; is</span>
<span class="sd">        only supported by the &#39;saga&#39; solver.</span>

<span class="sd">    scoring : str or callable, default=None</span>
<span class="sd">        A string (see model evaluation documentation) or</span>
<span class="sd">        a scorer callable object / function with signature</span>
<span class="sd">        ``scorer(estimator, X, y)``. For a list of scoring functions</span>
<span class="sd">        that can be used, look at :mod:`sklearn.metrics`. The</span>
<span class="sd">        default scoring option used is &#39;accuracy&#39;.</span>

<span class="sd">    solver : {&#39;newton-cg&#39;, &#39;lbfgs&#39;, &#39;liblinear&#39;, &#39;sag&#39;, &#39;saga&#39;}, \</span>
<span class="sd">            default=&#39;lbfgs&#39;</span>

<span class="sd">        Algorithm to use in the optimization problem.</span>

<span class="sd">        - For small datasets, &#39;liblinear&#39; is a good choice, whereas &#39;sag&#39; and</span>
<span class="sd">          &#39;saga&#39; are faster for large ones.</span>
<span class="sd">        - For multiclass problems, only &#39;newton-cg&#39;, &#39;sag&#39;, &#39;saga&#39; and &#39;lbfgs&#39;</span>
<span class="sd">          handle multinomial loss; &#39;liblinear&#39; is limited to one-versus-rest</span>
<span class="sd">          schemes.</span>
<span class="sd">        - &#39;newton-cg&#39;, &#39;lbfgs&#39; and &#39;sag&#39; only handle L2 penalty, whereas</span>
<span class="sd">          &#39;liblinear&#39; and &#39;saga&#39; handle L1 penalty.</span>
<span class="sd">        - &#39;liblinear&#39; might be slower in LogisticRegressionCV because it does</span>
<span class="sd">          not handle warm-starting.</span>

<span class="sd">        Note that &#39;sag&#39; and &#39;saga&#39; fast convergence is only guaranteed on</span>
<span class="sd">        features with approximately the same scale. You can preprocess the data</span>
<span class="sd">        with a scaler from sklearn.preprocessing.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           Stochastic Average Gradient descent solver.</span>
<span class="sd">        .. versionadded:: 0.19</span>
<span class="sd">           SAGA solver.</span>

<span class="sd">    tol : float, default=1e-4</span>
<span class="sd">        Tolerance for stopping criteria.</span>

<span class="sd">    max_iter : int, default=100</span>
<span class="sd">        Maximum number of iterations of the optimization algorithm.</span>

<span class="sd">    class_weight : dict or &#39;balanced&#39;, default=None</span>
<span class="sd">        Weights associated with classes in the form ``{class_label: weight}``.</span>
<span class="sd">        If not given, all classes are supposed to have weight one.</span>

<span class="sd">        The &quot;balanced&quot; mode uses the values of y to automatically adjust</span>
<span class="sd">        weights inversely proportional to class frequencies in the input data</span>
<span class="sd">        as ``n_samples / (n_classes * np.bincount(y))``.</span>

<span class="sd">        Note that these weights will be multiplied with sample_weight (passed</span>
<span class="sd">        through the fit method) if sample_weight is specified.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           class_weight == &#39;balanced&#39;</span>

<span class="sd">    n_jobs : int, default=None</span>
<span class="sd">        Number of CPU cores used during the cross-validation loop.</span>
<span class="sd">        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.</span>
<span class="sd">        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`</span>
<span class="sd">        for more details.</span>

<span class="sd">    verbose : int, default=0</span>
<span class="sd">        For the &#39;liblinear&#39;, &#39;sag&#39; and &#39;lbfgs&#39; solvers set verbose to any</span>
<span class="sd">        positive number for verbosity.</span>

<span class="sd">    refit : bool, default=True</span>
<span class="sd">        If set to True, the scores are averaged across all folds, and the</span>
<span class="sd">        coefs and the C that corresponds to the best score is taken, and a</span>
<span class="sd">        final refit is done using these parameters.</span>
<span class="sd">        Otherwise the coefs, intercepts and C that correspond to the</span>
<span class="sd">        best scores across folds are averaged.</span>

<span class="sd">    intercept_scaling : float, default=1</span>
<span class="sd">        Useful only when the solver &#39;liblinear&#39; is used</span>
<span class="sd">        and self.fit_intercept is set to True. In this case, x becomes</span>
<span class="sd">        [x, self.intercept_scaling],</span>
<span class="sd">        i.e. a &quot;synthetic&quot; feature with constant value equal to</span>
<span class="sd">        intercept_scaling is appended to the instance vector.</span>
<span class="sd">        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.</span>

<span class="sd">        Note! the synthetic feature weight is subject to l1/l2 regularization</span>
<span class="sd">        as all other features.</span>
<span class="sd">        To lessen the effect of regularization on synthetic feature weight</span>
<span class="sd">        (and therefore on the intercept) intercept_scaling has to be increased.</span>

<span class="sd">    multi_class : {&#39;auto, &#39;ovr&#39;, &#39;multinomial&#39;}, default=&#39;auto&#39;</span>
<span class="sd">        If the option chosen is &#39;ovr&#39;, then a binary problem is fit for each</span>
<span class="sd">        label. For &#39;multinomial&#39; the loss minimised is the multinomial loss fit</span>
<span class="sd">        across the entire probability distribution, *even when the data is</span>
<span class="sd">        binary*. &#39;multinomial&#39; is unavailable when solver=&#39;liblinear&#39;.</span>
<span class="sd">        &#39;auto&#39; selects &#39;ovr&#39; if the data is binary, or if solver=&#39;liblinear&#39;,</span>
<span class="sd">        and otherwise selects &#39;multinomial&#39;.</span>

<span class="sd">        .. versionadded:: 0.18</span>
<span class="sd">           Stochastic Average Gradient descent solver for &#39;multinomial&#39; case.</span>
<span class="sd">        .. versionchanged:: 0.22</span>
<span class="sd">            Default changed from &#39;ovr&#39; to &#39;auto&#39; in 0.22.</span>

<span class="sd">    random_state : int, RandomState instance, default=None</span>
<span class="sd">        Used when `solver=&#39;sag&#39;`, &#39;saga&#39; or &#39;liblinear&#39; to shuffle the data.</span>
<span class="sd">        Note that this only applies to the solver and not the cross-validation</span>
<span class="sd">        generator. See :term:`Glossary &lt;random_state&gt;` for details.</span>

<span class="sd">    l1_ratios : list of float, default=None</span>
<span class="sd">        The list of Elastic-Net mixing parameter, with ``0 &lt;= l1_ratio &lt;= 1``.</span>
<span class="sd">        Only used if ``penalty=&#39;elasticnet&#39;``. A value of 0 is equivalent to</span>
<span class="sd">        using ``penalty=&#39;l2&#39;``, while 1 is equivalent to using</span>
<span class="sd">        ``penalty=&#39;l1&#39;``. For ``0 &lt; l1_ratio &lt;1``, the penalty is a combination</span>
<span class="sd">        of L1 and L2.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    classes_ : ndarray of shape (n_classes, )</span>
<span class="sd">        A list of class labels known to the classifier.</span>

<span class="sd">    coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)</span>
<span class="sd">        Coefficient of the features in the decision function.</span>

<span class="sd">        `coef_` is of shape (1, n_features) when the given problem</span>
<span class="sd">        is binary.</span>

<span class="sd">    intercept_ : ndarray of shape (1,) or (n_classes,)</span>
<span class="sd">        Intercept (a.k.a. bias) added to the decision function.</span>

<span class="sd">        If `fit_intercept` is set to False, the intercept is set to zero.</span>
<span class="sd">        `intercept_` is of shape(1,) when the problem is binary.</span>

<span class="sd">    Cs_ : ndarray of shape (n_cs)</span>
<span class="sd">        Array of C i.e. inverse of regularization parameter values used</span>
<span class="sd">        for cross-validation.</span>

<span class="sd">    l1_ratios_ : ndarray of shape (n_l1_ratios)</span>
<span class="sd">        Array of l1_ratios used for cross-validation. If no l1_ratio is used</span>
<span class="sd">        (i.e. penalty is not &#39;elasticnet&#39;), this is set to ``[None]``</span>

<span class="sd">    coefs_paths_ : ndarray of shape (n_folds, n_cs, n_features) or \</span>
<span class="sd">                   (n_folds, n_cs, n_features + 1)</span>
<span class="sd">        dict with classes as the keys, and the path of coefficients obtained</span>
<span class="sd">        during cross-validating across each fold and then across each Cs</span>
<span class="sd">        after doing an OvR for the corresponding class as values.</span>
<span class="sd">        If the &#39;multi_class&#39; option is set to &#39;multinomial&#39;, then</span>
<span class="sd">        the coefs_paths are the coefficients corresponding to each class.</span>
<span class="sd">        Each dict value has shape ``(n_folds, n_cs, n_features)`` or</span>
<span class="sd">        ``(n_folds, n_cs, n_features + 1)`` depending on whether the</span>
<span class="sd">        intercept is fit or not. If ``penalty=&#39;elasticnet&#39;``, the shape is</span>
<span class="sd">        ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or</span>
<span class="sd">        ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``.</span>

<span class="sd">    scores_ : dict</span>
<span class="sd">        dict with classes as the keys, and the values as the</span>
<span class="sd">        grid of scores obtained during cross-validating each fold, after doing</span>
<span class="sd">        an OvR for the corresponding class. If the &#39;multi_class&#39; option</span>
<span class="sd">        given is &#39;multinomial&#39; then the same scores are repeated across</span>
<span class="sd">        all classes, since this is the multinomial class. Each dict value</span>
<span class="sd">        has shape ``(n_folds, n_cs`` or ``(n_folds, n_cs, n_l1_ratios)`` if</span>
<span class="sd">        ``penalty=&#39;elasticnet&#39;``.</span>

<span class="sd">    C_ : ndarray of shape (n_classes,) or (n_classes - 1,)</span>
<span class="sd">        Array of C that maps to the best scores across every class. If refit is</span>
<span class="sd">        set to False, then for each class, the best C is the average of the</span>
<span class="sd">        C&#39;s that correspond to the best scores for each fold.</span>
<span class="sd">        `C_` is of shape(n_classes,) when the problem is binary.</span>

<span class="sd">    l1_ratio_ : ndarray of shape (n_classes,) or (n_classes - 1,)</span>
<span class="sd">        Array of l1_ratio that maps to the best scores across every class. If</span>
<span class="sd">        refit is set to False, then for each class, the best l1_ratio is the</span>
<span class="sd">        average of the l1_ratio&#39;s that correspond to the best scores for each</span>
<span class="sd">        fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary.</span>

<span class="sd">    n_iter_ : ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)</span>
<span class="sd">        Actual number of iterations for all classes, folds and Cs.</span>
<span class="sd">        In the binary or multinomial cases, the first dimension is equal to 1.</span>
<span class="sd">        If ``penalty=&#39;elasticnet&#39;``, the shape is ``(n_classes, n_folds,</span>
<span class="sd">        n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``.</span>


<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.datasets import load_iris</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.linear_model import LogisticRegressionCV</span>
<span class="sd">    &gt;&gt;&gt; X, y = load_iris(return_X_y=True)</span>
<span class="sd">    &gt;&gt;&gt; clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)</span>
<span class="sd">    &gt;&gt;&gt; clf.predict(X[:2, :])</span>
<span class="sd">    array([0, 0])</span>
<span class="sd">    &gt;&gt;&gt; clf.predict_proba(X[:2, :]).shape</span>
<span class="sd">    (2, 3)</span>
<span class="sd">    &gt;&gt;&gt; clf.score(X, y)</span>
<span class="sd">    0.98...</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    LogisticRegression</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">Cs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
                 <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">refit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">intercept_scaling</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">l1_ratios</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Cs</span> <span class="o">=</span> <span class="n">Cs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">fit_intercept</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cv</span> <span class="o">=</span> <span class="n">cv</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dual</span> <span class="o">=</span> <span class="n">dual</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span> <span class="o">=</span> <span class="n">penalty</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scoring</span> <span class="o">=</span> <span class="n">scoring</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="o">=</span> <span class="n">class_weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">solver</span> <span class="o">=</span> <span class="n">solver</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">refit</span> <span class="o">=</span> <span class="n">refit</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intercept_scaling</span> <span class="o">=</span> <span class="n">intercept_scaling</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span> <span class="o">=</span> <span class="n">multi_class</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratios</span> <span class="o">=</span> <span class="n">l1_ratios</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit the model according to the given training data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Training vector, where n_samples is the number of samples and</span>
<span class="sd">            n_features is the number of features.</span>

<span class="sd">        y : array-like of shape (n_samples,)</span>
<span class="sd">            Target vector relative to X.</span>

<span class="sd">        sample_weight : array-like of shape (n_samples,) default=None</span>
<span class="sd">            Array of weights that are assigned to individual samples.</span>
<span class="sd">            If not provided, then each sample is given unit weight.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">solver</span> <span class="o">=</span> <span class="n">_check_solver</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">solver</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dual</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Maximum number of iteration must be positive;&quot;</span>
                             <span class="s2">&quot; got (max_iter=</span><span class="si">%r</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Tolerance for stopping criteria must be &quot;</span>
                             <span class="s2">&quot;positive; got (tol=</span><span class="si">%r</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span> <span class="o">==</span> <span class="s1">&#39;elasticnet&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratios</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1_ratios</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="nb">any</span><span class="p">(</span>
                    <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">l1_ratio</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)</span> <span class="ow">or</span> <span class="n">l1_ratio</span> <span class="o">&lt;</span> <span class="mi">0</span>
                     <span class="ow">or</span> <span class="n">l1_ratio</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">l1_ratio</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratios</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;l1_ratios must be a list of numbers between &quot;</span>
                                 <span class="s2">&quot;0 and 1; got (l1_ratios=</span><span class="si">%r</span><span class="s2">)&quot;</span> <span class="o">%</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratios</span><span class="p">)</span>
            <span class="n">l1_ratios_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratios</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratios</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;l1_ratios parameter is only used when penalty &quot;</span>
                              <span class="s2">&quot;is &#39;elasticnet&#39;. Got (penalty=</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                                  <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">))</span>

            <span class="n">l1_ratios_</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span> <span class="o">==</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;penalty=&#39;none&#39; is not useful and not supported by &quot;</span>
                <span class="s2">&quot;LogisticRegressionCV.&quot;</span>
            <span class="p">)</span>

        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                                   <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span>
                                   <span class="n">accept_large_sparse</span><span class="o">=</span><span class="n">solver</span> <span class="o">!=</span> <span class="s1">&#39;liblinear&#39;</span><span class="p">)</span>
        <span class="n">check_classification_targets</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="n">class_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span>

        <span class="c1"># Encode for string labels</span>
        <span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">class_weight</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">class_weight</span> <span class="o">=</span> <span class="p">{</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="bp">cls</span><span class="p">])[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">v</span>
                            <span class="k">for</span> <span class="bp">cls</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">class_weight</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

        <span class="c1"># The original class labels</span>
        <span class="n">classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">classes_</span>
        <span class="n">encoded_labels</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>

        <span class="n">multi_class</span> <span class="o">=</span> <span class="n">_check_multi_class</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span><span class="p">,</span> <span class="n">solver</span><span class="p">,</span>
                                         <span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">solver</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;sag&#39;</span><span class="p">,</span> <span class="s1">&#39;saga&#39;</span><span class="p">]:</span>
            <span class="n">max_squared_sum</span> <span class="o">=</span> <span class="n">row_norms</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">max_squared_sum</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># init cross-validation generator</span>
        <span class="n">cv</span> <span class="o">=</span> <span class="n">check_cv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">folds</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>

        <span class="c1"># Use the label encoded classes</span>
        <span class="n">n_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_labels</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">n_classes</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;This solver needs samples of at least 2 classes&quot;</span>
                             <span class="s2">&quot; in the data, but the data contains only one&quot;</span>
                             <span class="s2">&quot; class: </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">classes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">n_classes</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="c1"># OvR in case of binary problems is as good as fitting</span>
            <span class="c1"># the higher label</span>
            <span class="n">n_classes</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">encoded_labels</span> <span class="o">=</span> <span class="n">encoded_labels</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="n">classes</span> <span class="o">=</span> <span class="n">classes</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

        <span class="c1"># We need this hack to iterate only once over labels, in the case of</span>
        <span class="c1"># multi_class = multinomial, without changing the value of the labels.</span>
        <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
            <span class="n">iter_encoded_labels</span> <span class="o">=</span> <span class="n">iter_classes</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">iter_encoded_labels</span> <span class="o">=</span> <span class="n">encoded_labels</span>
            <span class="n">iter_classes</span> <span class="o">=</span> <span class="n">classes</span>

        <span class="c1"># compute the class weights for the entire dataset y</span>
        <span class="k">if</span> <span class="n">class_weight</span> <span class="o">==</span> <span class="s2">&quot;balanced&quot;</span><span class="p">:</span>
            <span class="n">class_weight</span> <span class="o">=</span> <span class="n">compute_class_weight</span><span class="p">(</span>
                <span class="n">class_weight</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)),</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
            <span class="n">class_weight</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">class_weight</span><span class="p">))</span>

        <span class="n">path_func</span> <span class="o">=</span> <span class="n">delayed</span><span class="p">(</span><span class="n">_log_reg_scoring_path</span><span class="p">)</span>

        <span class="c1"># The SAG solver releases the GIL so it&#39;s more efficient to use</span>
        <span class="c1"># threads for this solver.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;sag&#39;</span><span class="p">,</span> <span class="s1">&#39;saga&#39;</span><span class="p">]:</span>
            <span class="n">prefer</span> <span class="o">=</span> <span class="s1">&#39;threads&#39;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">prefer</span> <span class="o">=</span> <span class="s1">&#39;processes&#39;</span>

        <span class="n">fold_coefs_</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                               <span class="o">**</span><span class="n">_joblib_parallel_args</span><span class="p">(</span><span class="n">prefer</span><span class="o">=</span><span class="n">prefer</span><span class="p">))(</span>
            <span class="n">path_func</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="n">pos_class</span><span class="o">=</span><span class="n">label</span><span class="p">,</span> <span class="n">Cs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">Cs</span><span class="p">,</span>
                      <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span>
                      <span class="n">dual</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dual</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="n">solver</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span>
                      <span class="n">max_iter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                      <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weight</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scoring</span><span class="p">,</span>
                      <span class="n">multi_class</span><span class="o">=</span><span class="n">multi_class</span><span class="p">,</span>
                      <span class="n">intercept_scaling</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">intercept_scaling</span><span class="p">,</span>
                      <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
                      <span class="n">max_squared_sum</span><span class="o">=</span><span class="n">max_squared_sum</span><span class="p">,</span>
                      <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                      <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span>
                      <span class="p">)</span>
            <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">iter_encoded_labels</span>
            <span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">folds</span>
            <span class="k">for</span> <span class="n">l1_ratio</span> <span class="ow">in</span> <span class="n">l1_ratios_</span><span class="p">)</span>

        <span class="c1"># _log_reg_scoring_path will output different shapes depending on the</span>
        <span class="c1"># multi_class param, so we need to reshape the outputs accordingly.</span>
        <span class="c1"># Cs is of shape (n_classes . n_folds . n_l1_ratios, n_Cs) and all the</span>
        <span class="c1"># rows are equal, so we just take the first one.</span>
        <span class="c1"># After reshaping,</span>
        <span class="c1"># - scores is of shape (n_classes, n_folds, n_Cs . n_l1_ratios)</span>
        <span class="c1"># - coefs_paths is of shape</span>
        <span class="c1">#  (n_classes, n_folds, n_Cs . n_l1_ratios, n_features)</span>
        <span class="c1"># - n_iter is of shape</span>
        <span class="c1">#  (n_classes, n_folds, n_Cs . n_l1_ratios) or</span>
        <span class="c1">#  (1, n_folds, n_Cs . n_l1_ratios)</span>
        <span class="n">coefs_paths</span><span class="p">,</span> <span class="n">Cs</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">n_iter_</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">fold_coefs_</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Cs_</span> <span class="o">=</span> <span class="n">Cs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
            <span class="n">coefs_paths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">coefs_paths</span><span class="p">,</span>
                <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">folds</span><span class="p">),</span>  <span class="nb">len</span><span class="p">(</span><span class="n">l1_ratios_</span><span class="p">)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Cs_</span><span class="p">),</span> <span class="n">n_classes</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="c1"># equiv to coefs_paths = np.moveaxis(coefs_paths, (0, 1, 2, 3),</span>
            <span class="c1">#                                                 (1, 2, 0, 3))</span>
            <span class="n">coefs_paths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">coefs_paths</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">coefs_paths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">coefs_paths</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">n_iter_</span><span class="p">,</span>
                <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">folds</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Cs_</span><span class="p">)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">l1_ratios_</span><span class="p">))</span>
            <span class="p">)</span>
            <span class="c1"># repeat same scores across all classes</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">coefs_paths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">coefs_paths</span><span class="p">,</span>
                <span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">folds</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Cs_</span><span class="p">)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">l1_ratios_</span><span class="p">),</span>
                 <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">n_iter_</span><span class="p">,</span>
                <span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">folds</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Cs_</span><span class="p">)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">l1_ratios_</span><span class="p">))</span>
            <span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">folds</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scores_</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">classes</span><span class="p">,</span> <span class="n">scores</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coefs_paths_</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">classes</span><span class="p">,</span> <span class="n">coefs_paths</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">C_</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio_</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">encoded_label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                <span class="nb">zip</span><span class="p">(</span><span class="n">iter_classes</span><span class="p">,</span> <span class="n">iter_encoded_labels</span><span class="p">)):</span>

            <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;ovr&#39;</span><span class="p">:</span>
                <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scores_</span><span class="p">[</span><span class="bp">cls</span><span class="p">]</span>
                <span class="n">coefs_paths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coefs_paths_</span><span class="p">[</span><span class="bp">cls</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># For multinomial, all scores are the same across classes</span>
                <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="c1"># coefs_paths will keep its original shape because</span>
                <span class="c1"># logistic_regression_path expects it this way</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">refit</span><span class="p">:</span>
                <span class="c1"># best_index is between 0 and (n_Cs . n_l1_ratios - 1)</span>
                <span class="c1"># for example, with n_cs=2 and n_l1_ratios=3</span>
                <span class="c1"># the layout of scores is</span>
                <span class="c1"># [c1, c2, c1, c2, c1, c2]</span>
                <span class="c1">#   l1_1 ,  l1_2 ,  l1_3</span>
                <span class="n">best_index</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>

                <span class="n">best_index_C</span> <span class="o">=</span> <span class="n">best_index</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Cs_</span><span class="p">)</span>
                <span class="n">C_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Cs_</span><span class="p">[</span><span class="n">best_index_C</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">C_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">C_</span><span class="p">)</span>

                <span class="n">best_index_l1</span> <span class="o">=</span> <span class="n">best_index</span> <span class="o">//</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Cs_</span><span class="p">)</span>
                <span class="n">l1_ratio_</span> <span class="o">=</span> <span class="n">l1_ratios_</span><span class="p">[</span><span class="n">best_index_l1</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">l1_ratio_</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
                    <span class="n">coef_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">coefs_paths</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">best_index</span><span class="p">,</span> <span class="p">:],</span>
                                        <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">coef_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">coefs_paths</span><span class="p">[:,</span> <span class="n">best_index</span><span class="p">,</span> <span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

                <span class="c1"># Note that y is label encoded and hence pos_class must be</span>
                <span class="c1"># the encoded label / None (for &#39;multinomial&#39;)</span>
                <span class="n">w</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_logistic_regression_path</span><span class="p">(</span>
                    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pos_class</span><span class="o">=</span><span class="n">encoded_label</span><span class="p">,</span> <span class="n">Cs</span><span class="o">=</span><span class="p">[</span><span class="n">C_</span><span class="p">],</span> <span class="n">solver</span><span class="o">=</span><span class="n">solver</span><span class="p">,</span>
                    <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">,</span> <span class="n">coef</span><span class="o">=</span><span class="n">coef_init</span><span class="p">,</span>
                    <span class="n">max_iter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span>
                    <span class="n">penalty</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span>
                    <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weight</span><span class="p">,</span>
                    <span class="n">multi_class</span><span class="o">=</span><span class="n">multi_class</span><span class="p">,</span>
                    <span class="n">verbose</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
                    <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
                    <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_squared_sum</span><span class="o">=</span><span class="n">max_squared_sum</span><span class="p">,</span>
                    <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                    <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio_</span><span class="p">)</span>
                <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Take the best scores across every fold and the average of</span>
                <span class="c1"># all coefficients corresponding to the best scores.</span>
                <span class="n">best_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;ovr&#39;</span><span class="p">:</span>
                    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">coefs_paths</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">best_indices</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">:]</span>
                                 <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">folds</span><span class="p">))],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">coefs_paths</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="n">best_indices</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">:]</span>
                                 <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">folds</span><span class="p">))],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

                <span class="n">best_indices_C</span> <span class="o">=</span> <span class="n">best_indices</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Cs_</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">C_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Cs_</span><span class="p">[</span><span class="n">best_indices_C</span><span class="p">]))</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span> <span class="o">==</span> <span class="s1">&#39;elasticnet&#39;</span><span class="p">:</span>
                    <span class="n">best_indices_l1</span> <span class="o">=</span> <span class="n">best_indices</span> <span class="o">//</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Cs_</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">l1_ratios_</span><span class="p">[</span><span class="n">best_indices_l1</span><span class="p">]))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">C_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">C_</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio_</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">w</span><span class="p">[:,</span> <span class="p">:</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">w</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span><span class="p">[:</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">C_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">C_</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio_</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratios_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">l1_ratios_</span><span class="p">)</span>
        <span class="c1"># if elasticnet was used, add the l1_ratios dimension to some</span>
        <span class="c1"># attributes</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratios</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># with n_cs=2 and n_l1_ratios=3</span>
            <span class="c1"># the layout of scores is</span>
            <span class="c1"># [c1, c2, c1, c2, c1, c2]</span>
            <span class="c1">#   l1_1 ,  l1_2 ,  l1_3</span>
            <span class="c1"># To get a 2d array with the following layout</span>
            <span class="c1">#      l1_1, l1_2, l1_3</span>
            <span class="c1"># c1 [[ .  ,  .  ,  .  ],</span>
            <span class="c1"># c2  [ .  ,  .  ,  .  ]]</span>
            <span class="c1"># We need to first reshape and then transpose.</span>
            <span class="c1"># The same goes for the other arrays</span>
            <span class="k">for</span> <span class="bp">cls</span><span class="p">,</span> <span class="n">coefs_path</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">coefs_paths_</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coefs_paths_</span><span class="p">[</span><span class="bp">cls</span><span class="p">]</span> <span class="o">=</span> <span class="n">coefs_path</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                    <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">folds</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratios_</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Cs_</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coefs_paths_</span><span class="p">[</span><span class="bp">cls</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">coefs_paths_</span><span class="p">[</span><span class="bp">cls</span><span class="p">],</span>
                                                      <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
            <span class="k">for</span> <span class="bp">cls</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">scores_</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">scores_</span><span class="p">[</span><span class="bp">cls</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                    <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">folds</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratios_</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Cs_</span><span class="o">.</span><span class="n">size</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">scores_</span><span class="p">[</span><span class="bp">cls</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scores_</span><span class="p">[</span><span class="bp">cls</span><span class="p">],</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">folds</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratios_</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Cs_</span><span class="o">.</span><span class="n">size</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns the score using the `scoring` option on the given</span>
<span class="sd">        test data and labels.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like of shape (n_samples, n_features)</span>
<span class="sd">            Test samples.</span>

<span class="sd">        y : array-like of shape (n_samples,)</span>
<span class="sd">            True labels for X.</span>

<span class="sd">        sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        score : float</span>
<span class="sd">            Score of self.predict(X) wrt. y.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">scoring</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scoring</span> <span class="ow">or</span> <span class="s1">&#39;accuracy&#39;</span>
        <span class="n">scoring</span> <span class="o">=</span> <span class="n">get_scorer</span><span class="p">(</span><span class="n">scoring</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">scoring</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_more_tags</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;_xfail_checks&#39;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s1">&#39;check_sample_weights_invariance&#39;</span><span class="p">:</span>
                <span class="s1">&#39;zero sample_weight is not equivalent to removing samples&#39;</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="p">}</span>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, MIT Data To AI Lab.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>